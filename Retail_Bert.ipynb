{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0XEetYiXvRR",
        "outputId": "9d740b12-3855-4f96-d2a4-d1577b765f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (3371, 30), X_test shape: (843, 30)\n",
            "y_train shape: (3371,), y_test shape: (843,)\n",
            "Vocabulary Size: 3463\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\Online Retail.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Online Retail')\n",
        "\n",
        "# Cleaning\n",
        "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]\n",
        "\n",
        "# Encoding\n",
        "df_sorted = df_cleaned.sort_values(by=['CustomerID', 'InvoiceDate'])\n",
        "\n",
        "# Get all unique descriptions for fitting\n",
        "all_items = df_sorted['Description'].unique()\n",
        "\n",
        "# Fit LabelEncoder on all possible items\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(all_items)\n",
        "\n",
        "# Encode the ItemID column\n",
        "df_sorted['ItemID'] = item_encoder.transform(df_sorted['Description'])\n",
        "\n",
        "# Grouping by Customer\n",
        "sequential_data = df_sorted.groupby('CustomerID')['ItemID'].apply(list).reset_index(name='ItemSequence')\n",
        "\n",
        "# Minimum sequence length\n",
        "min_sequence_length = 3\n",
        "sequential_data = sequential_data[sequential_data['ItemSequence'].apply(len) >= min_sequence_length]\n",
        "\n",
        "# Pad and create sequences\n",
        "item_sequences = sequential_data['ItemSequence'].tolist()\n",
        "sequence_length = 30\n",
        "padded_sequences = pad_sequences(item_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "def create_sequences(sequences, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for seq in sequences:\n",
        "        for i in range(max(1, len(seq) - seq_length + 1)):\n",
        "            X.append(seq[i:i + seq_length])\n",
        "            y.append(seq[min(i + seq_length, len(seq) - 1)])  # Adjust for boundaries\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(padded_sequences, seq_length=sequence_length)\n",
        "\n",
        "# Dataset splitting\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Dynamically calculate the unique item count\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "unique_item_count = len(unique_items)\n",
        "\n",
        "# Fit LabelEncoder on all items in both X and y\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(unique_items)\n",
        "\n",
        "# Re-encode y to ensure compatibility with unique items\n",
        "y_train = item_encoder.transform(y_train)\n",
        "y_test = item_encoder.transform(y_test)\n",
        "\n",
        "# Ensure all labels are within the valid range\n",
        "assert max(y_train) < unique_item_count, \"y_train contains labels outside the valid range!\"\n",
        "assert max(y_test) < unique_item_count, \"y_test contains labels outside the valid range!\"\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"Vocabulary Size: {unique_item_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 5e-5\n",
        "sequence_length = 30\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpwBTvoFX0Oj",
        "outputId": "2e06fa80-a252-4e9b-c20f-ae0bbd85922c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Epoch 1/10 - 106/106 ━━━━━━ 0s remaining - loss: 7.6337\n",
            "Epoch 1/10 - Average Loss: 7.6337\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch 2/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.8552\n",
            "Epoch 2/10 - Average Loss: 6.8552\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch 3/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.7312\n",
            "Epoch 3/10 - Average Loss: 6.7312\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch 4/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.6954\n",
            "Epoch 4/10 - Average Loss: 6.6954\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch 5/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.6672\n",
            "Epoch 5/10 - Average Loss: 6.6672\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch 6/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.6178\n",
            "Epoch 6/10 - Average Loss: 6.6178\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch 7/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.5489\n",
            "Epoch 7/10 - Average Loss: 6.5489\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch 8/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.4848\n",
            "Epoch 8/10 - Average Loss: 6.4848\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch 9/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.4681\n",
            "Epoch 9/10 - Average Loss: 6.4681\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch 10/10 - 106/106 ━━━━━━ 0s remaining - loss: 6.4641\n",
            "Epoch 10/10 - Average Loss: 6.4641\n",
            "Precision@10: 0.0069, Recall@10: 0.0688, Hit Rate: 0.0688, MRR: 0.0228\n",
            "Results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 15\n",
        "learning_rate = 1e-5\n",
        "sequence_length = 30\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results2.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Md0iBDrkbRc",
        "outputId": "49578cc0-7e5f-4e76-8f92-9eed5fa94089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n",
            "Epoch 1/15 - 53/53 ━━━━━━ 0s remaining - loss: 8.0923\n",
            "Epoch 1/15 - Average Loss: 8.0923\n",
            "\n",
            "Epoch 2/15\n",
            "Epoch 2/15 - 53/53 ━━━━━━ 0s remaining - loss: 7.8102\n",
            "Epoch 2/15 - Average Loss: 7.8102\n",
            "\n",
            "Epoch 3/15\n",
            "Epoch 3/15 - 53/53 ━━━━━━ 0s remaining - loss: 7.5384\n",
            "Epoch 3/15 - Average Loss: 7.5384\n",
            "\n",
            "Epoch 4/15\n",
            "Epoch 4/15 - 53/53 ━━━━━━ 0s remaining - loss: 7.3227\n",
            "Epoch 4/15 - Average Loss: 7.3227\n",
            "\n",
            "Epoch 5/15\n",
            "Epoch 5/15 - 53/53 ━━━━━━ 0s remaining - loss: 7.1493\n",
            "Epoch 5/15 - Average Loss: 7.1493\n",
            "\n",
            "Epoch 6/15\n",
            "Epoch 6/15 - 53/53 ━━━━━━ 0s remaining - loss: 7.0205\n",
            "Epoch 6/15 - Average Loss: 7.0205\n",
            "\n",
            "Epoch 7/15\n",
            "Epoch 7/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.9169\n",
            "Epoch 7/15 - Average Loss: 6.9169\n",
            "\n",
            "Epoch 8/15\n",
            "Epoch 8/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.8369\n",
            "Epoch 8/15 - Average Loss: 6.8369\n",
            "\n",
            "Epoch 9/15\n",
            "Epoch 9/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.7661\n",
            "Epoch 9/15 - Average Loss: 6.7661\n",
            "\n",
            "Epoch 10/15\n",
            "Epoch 10/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.7088\n",
            "Epoch 10/15 - Average Loss: 6.7088\n",
            "\n",
            "Epoch 11/15\n",
            "Epoch 11/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.6523\n",
            "Epoch 11/15 - Average Loss: 6.6523\n",
            "\n",
            "Epoch 12/15\n",
            "Epoch 12/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.6086\n",
            "Epoch 12/15 - Average Loss: 6.6086\n",
            "\n",
            "Epoch 13/15\n",
            "Epoch 13/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.5650\n",
            "Epoch 13/15 - Average Loss: 6.5650\n",
            "\n",
            "Epoch 14/15\n",
            "Epoch 14/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.5232\n",
            "Epoch 14/15 - Average Loss: 6.5232\n",
            "\n",
            "Epoch 15/15\n",
            "Epoch 15/15 - 53/53 ━━━━━━ 0s remaining - loss: 6.4829\n",
            "Epoch 15/15 - Average Loss: 6.4829\n",
            "Precision@10: 0.0065, Recall@10: 0.0652, Hit Rate: 0.0652, MRR: 0.0288\n",
            "Results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "learning_rate = 1e-4\n",
        "sequence_length = 30\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results3.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTq5O9BdYMTr",
        "outputId": "36bdc490-d0cd-4d40-966e-1a7740dc1d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Epoch 1/20 - 106/106 ━━━━━━ 0s remaining - loss: 7.5435\n",
            "Epoch 1/20 - Average Loss: 7.5435\n",
            "\n",
            "Epoch 2/20\n",
            "Epoch 2/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.8188\n",
            "Epoch 2/20 - Average Loss: 6.8188\n",
            "\n",
            "Epoch 3/20\n",
            "Epoch 3/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.7298\n",
            "Epoch 3/20 - Average Loss: 6.7298\n",
            "\n",
            "Epoch 4/20\n",
            "Epoch 4/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6980\n",
            "Epoch 4/20 - Average Loss: 6.6980\n",
            "\n",
            "Epoch 5/20\n",
            "Epoch 5/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6801\n",
            "Epoch 5/20 - Average Loss: 6.6801\n",
            "\n",
            "Epoch 6/20\n",
            "Epoch 6/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6730\n",
            "Epoch 6/20 - Average Loss: 6.6730\n",
            "\n",
            "Epoch 7/20\n",
            "Epoch 7/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6597\n",
            "Epoch 7/20 - Average Loss: 6.6597\n",
            "\n",
            "Epoch 8/20\n",
            "Epoch 8/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6500\n",
            "Epoch 8/20 - Average Loss: 6.6500\n",
            "\n",
            "Epoch 9/20\n",
            "Epoch 9/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6562\n",
            "Epoch 9/20 - Average Loss: 6.6562\n",
            "\n",
            "Epoch 10/20\n",
            "Epoch 10/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6545\n",
            "Epoch 10/20 - Average Loss: 6.6545\n",
            "\n",
            "Epoch 11/20\n",
            "Epoch 11/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6413\n",
            "Epoch 11/20 - Average Loss: 6.6413\n",
            "\n",
            "Epoch 12/20\n",
            "Epoch 12/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6448\n",
            "Epoch 12/20 - Average Loss: 6.6448\n",
            "\n",
            "Epoch 13/20\n",
            "Epoch 13/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6435\n",
            "Epoch 13/20 - Average Loss: 6.6435\n",
            "\n",
            "Epoch 14/20\n",
            "Epoch 14/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6433\n",
            "Epoch 14/20 - Average Loss: 6.6433\n",
            "\n",
            "Epoch 15/20\n",
            "Epoch 15/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6371\n",
            "Epoch 15/20 - Average Loss: 6.6371\n",
            "\n",
            "Epoch 16/20\n",
            "Epoch 16/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6368\n",
            "Epoch 16/20 - Average Loss: 6.6368\n",
            "\n",
            "Epoch 17/20\n",
            "Epoch 17/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6359\n",
            "Epoch 17/20 - Average Loss: 6.6359\n",
            "\n",
            "Epoch 18/20\n",
            "Epoch 18/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6296\n",
            "Epoch 18/20 - Average Loss: 6.6296\n",
            "\n",
            "Epoch 19/20\n",
            "Epoch 19/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6361\n",
            "Epoch 19/20 - Average Loss: 6.6361\n",
            "\n",
            "Epoch 20/20\n",
            "Epoch 20/20 - 106/106 ━━━━━━ 0s remaining - loss: 6.6251\n",
            "Epoch 20/20 - Average Loss: 6.6251\n",
            "Precision@10: 0.0068, Recall@10: 0.0676, Hit Rate: 0.0676, MRR: 0.0226\n",
            "Results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\Online Retail.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Online Retail')\n",
        "\n",
        "# Cleaning\n",
        "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]\n",
        "\n",
        "# Encoding\n",
        "df_sorted = df_cleaned.sort_values(by=['CustomerID', 'InvoiceDate'])\n",
        "\n",
        "# Get all unique descriptions for fitting\n",
        "all_items = df_sorted['Description'].unique()\n",
        "\n",
        "# Fit LabelEncoder on all possible items\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(all_items)\n",
        "\n",
        "# Encode the ItemID column\n",
        "df_sorted['ItemID'] = item_encoder.transform(df_sorted['Description'])\n",
        "\n",
        "# Grouping by Customer\n",
        "sequential_data = df_sorted.groupby('CustomerID')['ItemID'].apply(list).reset_index(name='ItemSequence')\n",
        "\n",
        "# Minimum sequence length\n",
        "min_sequence_length = 3\n",
        "sequential_data = sequential_data[sequential_data['ItemSequence'].apply(len) >= min_sequence_length]\n",
        "\n",
        "# Pad and create sequences\n",
        "item_sequences = sequential_data['ItemSequence'].tolist()\n",
        "sequence_length = 15\n",
        "padded_sequences = pad_sequences(item_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "def create_sequences(sequences, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for seq in sequences:\n",
        "        for i in range(max(1, len(seq) - seq_length + 1)):\n",
        "            X.append(seq[i:i + seq_length])\n",
        "            y.append(seq[min(i + seq_length, len(seq) - 1)])  # Adjust for boundaries\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(padded_sequences, seq_length=sequence_length)\n",
        "\n",
        "# Dataset splitting\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Dynamically calculate the unique item count\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "unique_item_count = len(unique_items)\n",
        "\n",
        "# Fit LabelEncoder on all items in both X and y\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(unique_items)\n",
        "\n",
        "# Re-encode y to ensure compatibility with unique items\n",
        "y_train = item_encoder.transform(y_train)\n",
        "y_test = item_encoder.transform(y_test)\n",
        "\n",
        "# Ensure all labels are within the valid range\n",
        "assert max(y_train) < unique_item_count, \"y_train contains labels outside the valid range!\"\n",
        "assert max(y_test) < unique_item_count, \"y_test contains labels outside the valid range!\"\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"Vocabulary Size: {unique_item_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOJEeJaxoOWL",
        "outputId": "0e859576-c79d-4b0a-bdc1-c6efee0cfc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (3371, 15), X_test shape: (843, 15)\n",
            "y_train shape: (3371,), y_test shape: (843,)\n",
            "Vocabulary Size: 3201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "sequence_length = 15\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results4.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vuyXGJLkxQp",
        "outputId": "dc6700e3-23a5-493c-e57d-a5cc6c9cf58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/8\n",
            "Epoch 1/8 - 106/106 ━━━━━━ 0s remaining - loss: 7.6166\n",
            "Epoch 1/8 - Average Loss: 7.6166\n",
            "\n",
            "Epoch 2/8\n",
            "Epoch 2/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.8303\n",
            "Epoch 2/8 - Average Loss: 6.8303\n",
            "\n",
            "Epoch 3/8\n",
            "Epoch 3/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.7235\n",
            "Epoch 3/8 - Average Loss: 6.7235\n",
            "\n",
            "Epoch 4/8\n",
            "Epoch 4/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.6858\n",
            "Epoch 4/8 - Average Loss: 6.6858\n",
            "\n",
            "Epoch 5/8\n",
            "Epoch 5/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.6644\n",
            "Epoch 5/8 - Average Loss: 6.6644\n",
            "\n",
            "Epoch 6/8\n",
            "Epoch 6/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.6485\n",
            "Epoch 6/8 - Average Loss: 6.6485\n",
            "\n",
            "Epoch 7/8\n",
            "Epoch 7/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.6278\n",
            "Epoch 7/8 - Average Loss: 6.6278\n",
            "\n",
            "Epoch 8/8\n",
            "Epoch 8/8 - 106/106 ━━━━━━ 0s remaining - loss: 6.6000\n",
            "Epoch 8/8 - Average Loss: 6.6000\n",
            "Precision@10: 0.0063, Recall@10: 0.0629, Hit Rate: 0.0629, MRR: 0.0203\n",
            "Results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\Online Retail.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Online Retail')\n",
        "\n",
        "# Cleaning\n",
        "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]\n",
        "\n",
        "# Encoding\n",
        "df_sorted = df_cleaned.sort_values(by=['CustomerID', 'InvoiceDate'])\n",
        "\n",
        "# Get all unique descriptions for fitting\n",
        "all_items = df_sorted['Description'].unique()\n",
        "\n",
        "# Fit LabelEncoder on all possible items\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(all_items)\n",
        "\n",
        "# Encode the ItemID column\n",
        "df_sorted['ItemID'] = item_encoder.transform(df_sorted['Description'])\n",
        "\n",
        "# Grouping by Customer\n",
        "sequential_data = df_sorted.groupby('CustomerID')['ItemID'].apply(list).reset_index(name='ItemSequence')\n",
        "\n",
        "# Minimum sequence length\n",
        "min_sequence_length = 3\n",
        "sequential_data = sequential_data[sequential_data['ItemSequence'].apply(len) >= min_sequence_length]\n",
        "\n",
        "# Pad and create sequences\n",
        "item_sequences = sequential_data['ItemSequence'].tolist()\n",
        "sequence_length = 50\n",
        "padded_sequences = pad_sequences(item_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "def create_sequences(sequences, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for seq in sequences:\n",
        "        for i in range(max(1, len(seq) - seq_length + 1)):\n",
        "            X.append(seq[i:i + seq_length])\n",
        "            y.append(seq[min(i + seq_length, len(seq) - 1)])  # Adjust for boundaries\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(padded_sequences, seq_length=sequence_length)\n",
        "\n",
        "# Dataset splitting\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Dynamically calculate the unique item count\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "unique_item_count = len(unique_items)\n",
        "\n",
        "# Fit LabelEncoder on all items in both X and y\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(unique_items)\n",
        "\n",
        "# Re-encode y to ensure compatibility with unique items\n",
        "y_train = item_encoder.transform(y_train)\n",
        "y_test = item_encoder.transform(y_test)\n",
        "\n",
        "# Ensure all labels are within the valid range\n",
        "assert max(y_train) < unique_item_count, \"y_train contains labels outside the valid range!\"\n",
        "assert max(y_test) < unique_item_count, \"y_test contains labels outside the valid range!\"\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"Vocabulary Size: {unique_item_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6oZx-wsq4FZ",
        "outputId": "9a397b0b-0264-49d1-d042-ffdb9e51f072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (3371, 50), X_test shape: (843, 50)\n",
            "y_train shape: (3371,), y_test shape: (843,)\n",
            "Vocabulary Size: 3581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "learning_rate = 3e-5\n",
        "sequence_length = 50\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results5.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u_9Pnb3q6G1",
        "outputId": "499c39b2-cbc6-4467-c356-dce2e67053e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Epoch 1/10 - 53/53 ━━━━━━ 0s remaining - loss: 7.9491\n",
            "Epoch 1/10 - Average Loss: 7.9491\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch 2/10 - 53/53 ━━━━━━ 0s remaining - loss: 7.2850\n",
            "Epoch 2/10 - Average Loss: 7.2850\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch 3/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.9244\n",
            "Epoch 3/10 - Average Loss: 6.9244\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch 4/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.7692\n",
            "Epoch 4/10 - Average Loss: 6.7692\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch 5/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.6649\n",
            "Epoch 5/10 - Average Loss: 6.6649\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch 6/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.5925\n",
            "Epoch 6/10 - Average Loss: 6.5925\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch 7/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.5285\n",
            "Epoch 7/10 - Average Loss: 6.5285\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch 8/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.4726\n",
            "Epoch 8/10 - Average Loss: 6.4726\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch 9/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.4180\n",
            "Epoch 9/10 - Average Loss: 6.4180\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch 10/10 - 53/53 ━━━━━━ 0s remaining - loss: 6.3657\n",
            "Epoch 10/10 - Average Loss: 6.3657\n",
            "Precision@10: 0.0076, Recall@10: 0.0759, Hit Rate: 0.0759, MRR: 0.0322\n",
            "Results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\Online Retail.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Online Retail')\n",
        "\n",
        "# Cleaning\n",
        "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]\n",
        "\n",
        "# Encoding\n",
        "df_sorted = df_cleaned.sort_values(by=['CustomerID', 'InvoiceDate'])\n",
        "\n",
        "# Get all unique descriptions for fitting\n",
        "all_items = df_sorted['Description'].unique()\n",
        "\n",
        "# Fit LabelEncoder on all possible items\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(all_items)\n",
        "\n",
        "# Encode the ItemID column\n",
        "df_sorted['ItemID'] = item_encoder.transform(df_sorted['Description'])\n",
        "\n",
        "# Grouping by Customer\n",
        "sequential_data = df_sorted.groupby('CustomerID')['ItemID'].apply(list).reset_index(name='ItemSequence')\n",
        "\n",
        "# Minimum sequence length\n",
        "min_sequence_length = 3\n",
        "sequential_data = sequential_data[sequential_data['ItemSequence'].apply(len) >= min_sequence_length]\n",
        "\n",
        "# Pad and create sequences\n",
        "item_sequences = sequential_data['ItemSequence'].tolist()\n",
        "sequence_length = 40\n",
        "padded_sequences = pad_sequences(item_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "def create_sequences(sequences, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for seq in sequences:\n",
        "        for i in range(max(1, len(seq) - seq_length + 1)):\n",
        "            X.append(seq[i:i + seq_length])\n",
        "            y.append(seq[min(i + seq_length, len(seq) - 1)])  # Adjust for boundaries\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(padded_sequences, seq_length=sequence_length)\n",
        "\n",
        "# Dataset splitting\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Dynamically calculate the unique item count\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "unique_item_count = len(unique_items)\n",
        "\n",
        "# Fit LabelEncoder on all items in both X and y\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(unique_items)\n",
        "\n",
        "# Re-encode y to ensure compatibility with unique items\n",
        "y_train = item_encoder.transform(y_train)\n",
        "y_test = item_encoder.transform(y_test)\n",
        "\n",
        "# Ensure all labels are within the valid range\n",
        "assert max(y_train) < unique_item_count, \"y_train contains labels outside the valid range!\"\n",
        "assert max(y_test) < unique_item_count, \"y_test contains labels outside the valid range!\"\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"Vocabulary Size: {unique_item_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu3S53SLyiwc",
        "outputId": "1ecb614f-7edc-4e13-d024-7dfe78f5eb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (3371, 40), X_test shape: (843, 40)\n",
            "y_train shape: (3371,), y_test shape: (843,)\n",
            "Vocabulary Size: 3533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 3e-5\n",
        "sequence_length = 40\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results6.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2pJ_slXyjjB",
        "outputId": "b463a372-1a3c-41a6-a1fc-5969c66c3295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Epoch 1/20 - 53/53 ━━━━━━ 0s remaining - loss: 7.9619\n",
            "Epoch 1/20 - Average Loss: 7.9619\n",
            "\n",
            "Epoch 2/20\n",
            "Epoch 2/20 - 53/53 ━━━━━━ 0s remaining - loss: 7.3467\n",
            "Epoch 2/20 - Average Loss: 7.3467\n",
            "\n",
            "Epoch 3/20\n",
            "Epoch 3/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.9509\n",
            "Epoch 3/20 - Average Loss: 6.9509\n",
            "\n",
            "Epoch 4/20\n",
            "Epoch 4/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.7492\n",
            "Epoch 4/20 - Average Loss: 6.7492\n",
            "\n",
            "Epoch 5/20\n",
            "Epoch 5/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.6392\n",
            "Epoch 5/20 - Average Loss: 6.6392\n",
            "\n",
            "Epoch 6/20\n",
            "Epoch 6/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.5597\n",
            "Epoch 6/20 - Average Loss: 6.5597\n",
            "\n",
            "Epoch 7/20\n",
            "Epoch 7/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.4972\n",
            "Epoch 7/20 - Average Loss: 6.4972\n",
            "\n",
            "Epoch 8/20\n",
            "Epoch 8/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.4333\n",
            "Epoch 8/20 - Average Loss: 6.4333\n",
            "\n",
            "Epoch 9/20\n",
            "Epoch 9/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.3701\n",
            "Epoch 9/20 - Average Loss: 6.3701\n",
            "\n",
            "Epoch 10/20\n",
            "Epoch 10/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.3155\n",
            "Epoch 10/20 - Average Loss: 6.3155\n",
            "\n",
            "Epoch 11/20\n",
            "Epoch 11/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.2617\n",
            "Epoch 11/20 - Average Loss: 6.2617\n",
            "\n",
            "Epoch 12/20\n",
            "Epoch 12/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.1875\n",
            "Epoch 12/20 - Average Loss: 6.1875\n",
            "\n",
            "Epoch 13/20\n",
            "Epoch 13/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.1193\n",
            "Epoch 13/20 - Average Loss: 6.1193\n",
            "\n",
            "Epoch 14/20\n",
            "Epoch 14/20 - 53/53 ━━━━━━ 0s remaining - loss: 6.0356\n",
            "Epoch 14/20 - Average Loss: 6.0356\n",
            "\n",
            "Epoch 15/20\n",
            "Epoch 15/20 - 53/53 ━━━━━━ 0s remaining - loss: 5.9545\n",
            "Epoch 15/20 - Average Loss: 5.9545\n",
            "\n",
            "Epoch 16/20\n",
            "Epoch 16/20 - 53/53 ━━━━━━ 0s remaining - loss: 5.8802\n",
            "Epoch 16/20 - Average Loss: 5.8802\n",
            "\n",
            "Epoch 17/20\n",
            "Epoch 17/20 - 53/53 ━━━━━━ 0s remaining - loss: 5.7906\n",
            "Epoch 17/20 - Average Loss: 5.7906\n",
            "\n",
            "Epoch 18/20\n",
            "Epoch 18/20 - 53/53 ━━━━━━ 0s remaining - loss: 5.7045\n",
            "Epoch 18/20 - Average Loss: 5.7045\n",
            "\n",
            "Epoch 19/20\n",
            "Epoch 19/20 - 53/53 ━━━━━━ 0s remaining - loss: 5.6272\n",
            "Epoch 19/20 - Average Loss: 5.6272\n",
            "\n",
            "Epoch 20/20\n",
            "Epoch 20/20 - 53/53 ━━━━━━ 0s remaining - loss: 5.5268\n",
            "Epoch 20/20 - Average Loss: 5.5268\n",
            "Precision@10: 0.0057, Recall@10: 0.0569, Hit Rate: 0.0569, MRR: 0.0265\n",
            "Results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\Online Retail.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Online Retail')\n",
        "\n",
        "# Cleaning\n",
        "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]\n",
        "\n",
        "# Encoding\n",
        "df_sorted = df_cleaned.sort_values(by=['CustomerID', 'InvoiceDate'])\n",
        "\n",
        "# Get all unique descriptions for fitting\n",
        "all_items = df_sorted['Description'].unique()\n",
        "\n",
        "# Fit LabelEncoder on all possible items\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(all_items)\n",
        "\n",
        "# Encode the ItemID column\n",
        "df_sorted['ItemID'] = item_encoder.transform(df_sorted['Description'])\n",
        "\n",
        "# Grouping by Customer\n",
        "sequential_data = df_sorted.groupby('CustomerID')['ItemID'].apply(list).reset_index(name='ItemSequence')\n",
        "\n",
        "# Minimum sequence length\n",
        "min_sequence_length = 3\n",
        "sequential_data = sequential_data[sequential_data['ItemSequence'].apply(len) >= min_sequence_length]\n",
        "\n",
        "# Pad and create sequences\n",
        "item_sequences = sequential_data['ItemSequence'].tolist()\n",
        "sequence_length = 50\n",
        "padded_sequences = pad_sequences(item_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "def create_sequences(sequences, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for seq in sequences:\n",
        "        for i in range(max(1, len(seq) - seq_length + 1)):\n",
        "            X.append(seq[i:i + seq_length])\n",
        "            y.append(seq[min(i + seq_length, len(seq) - 1)])  # Adjust for boundaries\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(padded_sequences, seq_length=sequence_length)\n",
        "\n",
        "# Dataset splitting\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Dynamically calculate the unique item count\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "unique_item_count = len(unique_items)\n",
        "\n",
        "# Fit LabelEncoder on all items in both X and y\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(unique_items)\n",
        "\n",
        "# Re-encode y to ensure compatibility with unique items\n",
        "y_train = item_encoder.transform(y_train)\n",
        "y_test = item_encoder.transform(y_test)\n",
        "\n",
        "# Ensure all labels are within the valid range\n",
        "assert max(y_train) < unique_item_count, \"y_train contains labels outside the valid range!\"\n",
        "assert max(y_test) < unique_item_count, \"y_test contains labels outside the valid range!\"\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"Vocabulary Size: {unique_item_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVOXGC-HorGr",
        "outputId": "d2a7dba6-a688-444b-e197-b91c95e8f1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (3371, 50), X_test shape: (843, 50)\n",
            "y_train shape: (3371,), y_test shape: (843,)\n",
            "Vocabulary Size: 3581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# CPU Optimization\n",
        "torch.set_num_threads(16)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "learning_rate = 1e-5\n",
        "sequence_length = 50\n",
        "k = 10\n",
        "\n",
        "# BERT Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=unique_item_count,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Prepare DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.inputs['input_ids'][idx],\n",
        "            'attention_mask': self.inputs['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare inputs for BERT\n",
        "def prepare_inputs(X):\n",
        "    return [\" \".join(map(str, seq)) for seq in X]\n",
        "\n",
        "X_train_str = prepare_inputs(X_train)\n",
        "X_test_str = prepare_inputs(X_test)\n",
        "\n",
        "train_encodings = tokenizer(X_train_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "test_encodings = tokenizer(X_test_str, truncation=True, padding=True, max_length=sequence_length, return_tensors='pt')\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, y_train)\n",
        "test_dataset = CustomDataset(test_encodings, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training Function with Epoch Display\n",
        "def train_model(model, train_loader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    device = torch.device('cpu')  # Use CPU\n",
        "    model.to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    steps_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Countdown Display\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_remaining = steps_per_epoch - step\n",
        "            time_per_step = elapsed_time / max(1, step)\n",
        "            estimated_time_remaining = steps_remaining * time_per_step\n",
        "\n",
        "            print(\n",
        "                f\"\\rEpoch {epoch + 1}/{epochs} - {step}/{steps_per_epoch} ━━━━━━ \"\n",
        "                f\"{int(estimated_time_remaining)}s remaining - loss: {total_loss / step:.4f}\", end=\"\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs} - Average Loss: {total_loss / steps_per_epoch:.4f}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, y_true, k=10):\n",
        "    model.eval()\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    total_precision, total_recall, total_hits, total_mrr = 0, 0, 0, 0\n",
        "    total_users = len(y_true)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, top_k_indices = torch.topk(logits, k, dim=1)\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                predictions = top_k_indices[idx].cpu().numpy()\n",
        "                label = label.item()\n",
        "                if label in predictions:\n",
        "                    rank = np.where(predictions == label)[0][0] + 1\n",
        "                    total_hits += 1\n",
        "                    total_mrr += 1 / rank\n",
        "\n",
        "                precision_k = len(set(predictions) & {label}) / k\n",
        "                recall_k = len(set(predictions) & {label}) / 1\n",
        "                total_precision += precision_k\n",
        "                total_recall += recall_k\n",
        "\n",
        "    precision = total_precision / total_users\n",
        "    recall = total_recall / total_users\n",
        "    hit_rate = total_hits / total_users\n",
        "    mrr = total_mrr / total_users\n",
        "\n",
        "    print(f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}, Hit Rate: {hit_rate:.4f}, MRR: {mrr:.4f}\")\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate the Model\n",
        "precision, recall, hit_rate, mrr = evaluate_model(model, test_loader, y_test, k=k)\n",
        "\n",
        "# Save Results\n",
        "results = pd.DataFrame([{\n",
        "    \"Precision@10\": precision,\n",
        "    \"Recall@10\": recall,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}])\n",
        "results.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\bert4rec_results7.csv\", index=False)\n",
        "print(\"Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxtxiLoYyk82",
        "outputId": "fd414f0f-ac05-440c-a7d4-5a9b83c59e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n",
            "Epoch 1/15 - 106/106 ━━━━━━ 0s remaining - loss: 8.0777\n",
            "Epoch 1/15 - Average Loss: 8.0777\n",
            "\n",
            "Epoch 2/15\n",
            "Epoch 2/15 - 106/106 ━━━━━━ 0s remaining - loss: 7.5810\n",
            "Epoch 2/15 - Average Loss: 7.5810\n",
            "\n",
            "Epoch 3/15\n",
            "Epoch 3/15 - 106/106 ━━━━━━ 0s remaining - loss: 7.1895\n",
            "Epoch 3/15 - Average Loss: 7.1895\n",
            "\n",
            "Epoch 4/15\n",
            "Epoch 4/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.9770\n",
            "Epoch 4/15 - Average Loss: 6.9770\n",
            "\n",
            "Epoch 5/15\n",
            "Epoch 5/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.8323\n",
            "Epoch 5/15 - Average Loss: 6.8323\n",
            "\n",
            "Epoch 6/15\n",
            "Epoch 6/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.7372\n",
            "Epoch 6/15 - Average Loss: 6.7372\n",
            "\n",
            "Epoch 7/15\n",
            "Epoch 7/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.6699\n",
            "Epoch 7/15 - Average Loss: 6.6699\n",
            "\n",
            "Epoch 8/15\n",
            "Epoch 8/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.6096\n",
            "Epoch 8/15 - Average Loss: 6.6096\n",
            "\n",
            "Epoch 9/15\n",
            "Epoch 9/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.5790\n",
            "Epoch 9/15 - Average Loss: 6.5790\n",
            "\n",
            "Epoch 10/15\n",
            "Epoch 10/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.5463\n",
            "Epoch 10/15 - Average Loss: 6.5463\n",
            "\n",
            "Epoch 11/15\n",
            "Epoch 11/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.5043\n",
            "Epoch 11/15 - Average Loss: 6.5043\n",
            "\n",
            "Epoch 12/15\n",
            "Epoch 12/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.4938\n",
            "Epoch 12/15 - Average Loss: 6.4938\n",
            "\n",
            "Epoch 13/15\n",
            "Epoch 13/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.4589\n",
            "Epoch 13/15 - Average Loss: 6.4589\n",
            "\n",
            "Epoch 14/15\n",
            "Epoch 14/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.4397\n",
            "Epoch 14/15 - Average Loss: 6.4397\n",
            "\n",
            "Epoch 15/15\n",
            "Epoch 15/15 - 106/106 ━━━━━━ 0s remaining - loss: 6.3995\n",
            "Epoch 15/15 - Average Loss: 6.3995\n",
            "Precision@10: 0.0070, Recall@10: 0.0700, Hit Rate: 0.0700, MRR: 0.0342\n",
            "Results saved.\n"
          ]
        }
      ]
    }
  ]
}