{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization based on the i7-13700K capabilities\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function (modify based on dataset specifics)\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    # Merge orders with prior products to get sequences per user\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    # Group by user_id to create sequences of product_ids\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "\n",
        "    # Ensure sequences are not empty, otherwise filter them out\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]  # Take the last item as the label\n",
        "\n",
        "    # Pad sequences to ensure they have the same length\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1  # Example value based on products dataset\n",
        "embedding_dim = 50  # Example value\n",
        "hidden_units = 100  # Example value\n",
        "max_sequence_length = 200  # Example value\n",
        "\n",
        "# Preprocess the training and testing data\n",
        "X_train, y_train = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_test, y_test = preprocess_data(orders, order_products_prior, max_sequence_length)  # Adjust as necessary for testing data\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "        GRU(hidden_units, return_sequences=False),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32  # Example value\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the GRU4Rec model with reduced epochs for quicker execution\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=5,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "# Evaluation function for the GRU4Rec model\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, count = 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            if true_item in top_k_preds[i]:\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    return precision, recall, hit_rate\n",
        "\n",
        "# Evaluate GRU4Rec model\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@5, Recall@5, Hit Rate):\", gru4rec_results)\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "#        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4recNO1_results.csv')\n"
      ],
      "metadata": {
        "id": "Agbd2JLYNUsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b468aae-035d-462b-8dbf-1a95e838b7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6444/6444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 69ms/step - accuracy: 0.0116 - loss: 8.7436 - val_accuracy: 0.0845 - val_loss: 6.2161\n",
            "Epoch 2/5\n",
            "\u001b[1m6444/6444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 68ms/step - accuracy: 0.1288 - loss: 6.0016 - val_accuracy: 0.3485 - val_loss: 4.4276\n",
            "Epoch 3/5\n",
            "\u001b[1m6444/6444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 66ms/step - accuracy: 0.4366 - loss: 3.8422 - val_accuracy: 0.6527 - val_loss: 2.5299\n",
            "Epoch 4/5\n",
            "\u001b[1m6444/6444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 65ms/step - accuracy: 0.6686 - loss: 2.3601 - val_accuracy: 0.7914 - val_loss: 1.6455\n",
            "Epoch 5/5\n",
            "\u001b[1m6444/6444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 65ms/step - accuracy: 0.7904 - loss: 1.5374 - val_accuracy: 0.8593 - val_loss: 1.1731\n",
            "GRU4Rec Performance (Precision@5, Recall@5, Hit Rate): (0.89754034761018, 0.89754034761018, 0.89754034761018)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization based on the i7-13700K capabilities\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function (modify based on dataset specifics)\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    # Merge orders with prior products to get sequences per user\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    # Group by user_id to create sequences of product_ids\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "\n",
        "    # Ensure sequences are not empty, otherwise filter them out\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]  # Take the last item as the label\n",
        "\n",
        "    # Pad sequences to ensure they have the same length\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1  # Example value based on products dataset\n",
        "embedding_dim = 100  # Adjusted for better representation\n",
        "hidden_units = 128  # Adjusted hidden units\n",
        "max_sequence_length = 100  # Example value\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32  # Example value\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the GRU4Rec model with more epochs for better learning\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=10,  # Increased number of epochs\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function for the GRU4Rec model with additional metrics\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate GRU4Rec model\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_resultsO1.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNuC3vDbhzjh",
        "outputId": "0bade9d7-1bf7-4836-8699-0bde0cb4e38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 59ms/step - accuracy: 0.0230 - loss: 8.4617 - val_accuracy: 0.2189 - val_loss: 5.6836\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 61ms/step - accuracy: 0.3461 - loss: 4.5928 - val_accuracy: 0.6794 - val_loss: 2.9802\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 61ms/step - accuracy: 0.7414 - loss: 2.0019 - val_accuracy: 0.8152 - val_loss: 2.0455\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 62ms/step - accuracy: 0.8749 - loss: 0.9765 - val_accuracy: 0.8603 - val_loss: 1.7018\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 62ms/step - accuracy: 0.9393 - loss: 0.5294 - val_accuracy: 0.8843 - val_loss: 1.5302\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 60ms/step - accuracy: 0.9694 - loss: 0.3008 - val_accuracy: 0.8936 - val_loss: 1.4197\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 61ms/step - accuracy: 0.9815 - loss: 0.1924 - val_accuracy: 0.9001 - val_loss: 1.3432\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 61ms/step - accuracy: 0.9893 - loss: 0.1409 - val_accuracy: 0.9050 - val_loss: 1.2977\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 61ms/step - accuracy: 0.9939 - loss: 0.1065 - val_accuracy: 0.9077 - val_loss: 1.2707\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 65ms/step - accuracy: 0.9964 - loss: 0.0912 - val_accuracy: 0.9109 - val_loss: 1.2467\n",
            "GRU4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR): (0.922166149068323, 0.922166149068323, 0.922166149068323, 0.18676403985495857)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "hidden_units = 250\n",
        "max_sequence_length = 20\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False, dropout=0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    return dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=10):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate and save results\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "def save_results_to_csv(results, base_path, filename, hyperparameters):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@10', results[0]])\n",
        "        writer.writerow(['Recall@10', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(['Hyperparameters'])\n",
        "        for key, value in hyperparameters.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "# Save the results with hyperparameters\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_units': hidden_units,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'learning_rate': learning_rate\n",
        "}\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_results_with_hyperparameters1.csv', hyperparameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_UVlJQYLuVp",
        "outputId": "4a342a1f-e90e-427f-89f4-9fa8bfaf9dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 67ms/step - accuracy: 0.0097 - loss: 9.1440 - val_accuracy: 0.0122 - val_loss: 8.0438\n",
            "Epoch 2/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 68ms/step - accuracy: 0.0191 - loss: 7.6132 - val_accuracy: 0.0453 - val_loss: 7.5382\n",
            "Epoch 3/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 69ms/step - accuracy: 0.0660 - loss: 6.8836 - val_accuracy: 0.1345 - val_loss: 6.8614\n",
            "Epoch 4/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 69ms/step - accuracy: 0.1546 - loss: 6.0701 - val_accuracy: 0.2368 - val_loss: 6.2011\n",
            "Epoch 5/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 69ms/step - accuracy: 0.2598 - loss: 5.2979 - val_accuracy: 0.3404 - val_loss: 5.5809\n",
            "Epoch 6/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 68ms/step - accuracy: 0.3729 - loss: 4.5826 - val_accuracy: 0.4333 - val_loss: 5.0183\n",
            "Epoch 7/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 69ms/step - accuracy: 0.4751 - loss: 3.9377 - val_accuracy: 0.5122 - val_loss: 4.4984\n",
            "Epoch 8/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 69ms/step - accuracy: 0.5644 - loss: 3.3695 - val_accuracy: 0.5767 - val_loss: 4.0613\n",
            "Epoch 9/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 68ms/step - accuracy: 0.6390 - loss: 2.8873 - val_accuracy: 0.6259 - val_loss: 3.6908\n",
            "Epoch 10/10\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 67ms/step - accuracy: 0.7036 - loss: 2.4747 - val_accuracy: 0.6682 - val_loss: 3.3738\n",
            "GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR): (0.7287301125339543, 0.7287301125339543, 0.7287301125339543, 0.07976040338522081)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "hidden_units = 200\n",
        "max_sequence_length = 20\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False, dropout=0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    return dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=10):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate and save results\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "def save_results_to_csv(results, base_path, filename, hyperparameters):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@10', results[0]])\n",
        "        writer.writerow(['Recall@10', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(['Hyperparameters'])\n",
        "        for key, value in hyperparameters.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "# Save the results with hyperparameters\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_units': hidden_units,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'learning_rate': learning_rate\n",
        "}\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_results_with_hyperparameters2.csv', hyperparameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apiYhLT3e4W2",
        "outputId": "f6ea1a9e-8ccf-4280-8262-d3b948de90f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 80ms/step - accuracy: 0.0109 - loss: 9.3020 - val_accuracy: 0.0084 - val_loss: 8.2223\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 81ms/step - accuracy: 0.0119 - loss: 7.8640 - val_accuracy: 0.0165 - val_loss: 7.9627\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 80ms/step - accuracy: 0.0223 - loss: 7.4855 - val_accuracy: 0.0321 - val_loss: 7.7477\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 81ms/step - accuracy: 0.0386 - loss: 7.1260 - val_accuracy: 0.0565 - val_loss: 7.5416\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 82ms/step - accuracy: 0.0645 - loss: 6.7931 - val_accuracy: 0.1000 - val_loss: 7.2929\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 82ms/step - accuracy: 0.1100 - loss: 6.4156 - val_accuracy: 0.1474 - val_loss: 6.9530\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 82ms/step - accuracy: 0.1597 - loss: 6.0055 - val_accuracy: 0.1949 - val_loss: 6.6131\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 82ms/step - accuracy: 0.2120 - loss: 5.5928 - val_accuracy: 0.2523 - val_loss: 6.2751\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 82ms/step - accuracy: 0.2733 - loss: 5.1866 - val_accuracy: 0.3156 - val_loss: 5.9280\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 82ms/step - accuracy: 0.3411 - loss: 4.7760 - val_accuracy: 0.3766 - val_loss: 5.5667\n",
            "GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR): (0.4954386645962733, 0.4954386645962733, 0.4954386645962733, 0.06303050941842393)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "hidden_units = 200\n",
        "max_sequence_length = 20\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False, dropout=0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    return dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=10):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate and save results\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "def save_results_to_csv(results, base_path, filename, hyperparameters):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@10', results[0]])\n",
        "        writer.writerow(['Recall@10', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(['Hyperparameters'])\n",
        "        for key, value in hyperparameters.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "# Save the results with hyperparameters\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_units': hidden_units,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'learning_rate': learning_rate\n",
        "}\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_results_with_hyperparameters3.csv', hyperparameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnSmvzh0fFfX",
        "outputId": "6a03a81f-eb69-4f7a-d0ed-2153edb455e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 84ms/step - accuracy: 0.0268 - loss: 8.4870 - val_accuracy: 0.2386 - val_loss: 5.8008\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 83ms/step - accuracy: 0.3336 - loss: 4.8940 - val_accuracy: 0.6249 - val_loss: 3.3515\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 83ms/step - accuracy: 0.6823 - loss: 2.4929 - val_accuracy: 0.7841 - val_loss: 2.2517\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 83ms/step - accuracy: 0.8423 - loss: 1.3064 - val_accuracy: 0.8473 - val_loss: 1.8066\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 83ms/step - accuracy: 0.9250 - loss: 0.7264 - val_accuracy: 0.8753 - val_loss: 1.5701\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 83ms/step - accuracy: 0.9714 - loss: 0.4075 - val_accuracy: 0.8914 - val_loss: 1.4244\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 83ms/step - accuracy: 0.9918 - loss: 0.2258 - val_accuracy: 0.9023 - val_loss: 1.3417\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 83ms/step - accuracy: 0.9987 - loss: 0.1210 - val_accuracy: 0.9086 - val_loss: 1.2836\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 83ms/step - accuracy: 0.9999 - loss: 0.0593 - val_accuracy: 0.9160 - val_loss: 1.2284\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0274 - val_accuracy: 0.9209 - val_loss: 1.1799\n",
            "GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR): (0.9342973602484472, 0.9342973602484472, 0.9342973602484472, 0.094600060925581)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "hidden_units = 300\n",
        "max_sequence_length = 20\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.0003\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False, dropout=0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    return dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=10):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate and save results\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "def save_results_to_csv(results, base_path, filename, hyperparameters):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@10', results[0]])\n",
        "        writer.writerow(['Recall@10', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(['Hyperparameters'])\n",
        "        for key, value in hyperparameters.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "# Save the results with hyperparameters\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_units': hidden_units,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'learning_rate': learning_rate\n",
        "}\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_results_with_hyperparameters4.csv', hyperparameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvJTsNtNfPd3",
        "outputId": "e859c897-f32a-4874-98b1-f892eb95c7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 118ms/step - accuracy: 0.0157 - loss: 8.7723 - val_accuracy: 0.0996 - val_loss: 6.7609\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 117ms/step - accuracy: 0.1613 - loss: 6.0260 - val_accuracy: 0.3911 - val_loss: 4.9332\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 117ms/step - accuracy: 0.4499 - loss: 3.9472 - val_accuracy: 0.6112 - val_loss: 3.5356\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 117ms/step - accuracy: 0.6715 - loss: 2.4293 - val_accuracy: 0.7256 - val_loss: 2.7443\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 117ms/step - accuracy: 0.8149 - loss: 1.4928 - val_accuracy: 0.7879 - val_loss: 2.2996\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 116ms/step - accuracy: 0.9102 - loss: 0.9112 - val_accuracy: 0.8223 - val_loss: 2.0238\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 116ms/step - accuracy: 0.9654 - loss: 0.5496 - val_accuracy: 0.8435 - val_loss: 1.8509\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 114ms/step - accuracy: 0.9906 - loss: 0.3225 - val_accuracy: 0.8570 - val_loss: 1.7309\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m599s\u001b[0m 116ms/step - accuracy: 0.9981 - loss: 0.1785 - val_accuracy: 0.8673 - val_loss: 1.6244\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 116ms/step - accuracy: 0.9995 - loss: 0.0958 - val_accuracy: 0.8763 - val_loss: 1.5419\n",
            "GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR): (0.9001843944099379, 0.9001843944099379, 0.9001843944099379, 0.09252646144503171)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "hidden_units = 250\n",
        "max_sequence_length = 20\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False, dropout=0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    return dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=10):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate and save results\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "def save_results_to_csv(results, base_path, filename, hyperparameters):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@10', results[0]])\n",
        "        writer.writerow(['Recall@10', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(['Hyperparameters'])\n",
        "        for key, value in hyperparameters.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "# Save the results with hyperparameters\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_units': hidden_units,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'learning_rate': learning_rate\n",
        "}\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_results_with_hyperparameters5.csv', hyperparameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK23F42ofWXg",
        "outputId": "1fd6588b-e430-4a66-fb42-9c0fb5e73bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m926s\u001b[0m 90ms/step - accuracy: 0.0107 - loss: 9.1493 - val_accuracy: 0.0169 - val_loss: 8.0542\n",
            "Epoch 2/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 89ms/step - accuracy: 0.0256 - loss: 7.6086 - val_accuracy: 0.0522 - val_loss: 7.5204\n",
            "Epoch 3/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 89ms/step - accuracy: 0.0699 - loss: 6.8674 - val_accuracy: 0.1395 - val_loss: 6.8591\n",
            "Epoch 4/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 89ms/step - accuracy: 0.1580 - loss: 6.0497 - val_accuracy: 0.2348 - val_loss: 6.1979\n",
            "Epoch 5/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m919s\u001b[0m 89ms/step - accuracy: 0.2606 - loss: 5.2825 - val_accuracy: 0.3427 - val_loss: 5.5973\n",
            "Epoch 6/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 89ms/step - accuracy: 0.3751 - loss: 4.5735 - val_accuracy: 0.4371 - val_loss: 5.0281\n",
            "Epoch 7/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 89ms/step - accuracy: 0.4813 - loss: 3.9110 - val_accuracy: 0.5159 - val_loss: 4.4780\n",
            "Epoch 8/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 89ms/step - accuracy: 0.5742 - loss: 3.3189 - val_accuracy: 0.5851 - val_loss: 4.0196\n",
            "Epoch 9/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 89ms/step - accuracy: 0.6546 - loss: 2.8162 - val_accuracy: 0.6363 - val_loss: 3.6325\n",
            "Epoch 10/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 89ms/step - accuracy: 0.7206 - loss: 2.3908 - val_accuracy: 0.6792 - val_loss: 3.3062\n",
            "Epoch 11/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m911s\u001b[0m 88ms/step - accuracy: 0.7770 - loss: 2.0469 - val_accuracy: 0.7122 - val_loss: 3.0543\n",
            "Epoch 12/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m908s\u001b[0m 88ms/step - accuracy: 0.8217 - loss: 1.7567 - val_accuracy: 0.7384 - val_loss: 2.8461\n",
            "Epoch 13/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 88ms/step - accuracy: 0.8600 - loss: 1.5234 - val_accuracy: 0.7605 - val_loss: 2.6768\n",
            "Epoch 14/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m904s\u001b[0m 88ms/step - accuracy: 0.8866 - loss: 1.3395 - val_accuracy: 0.7797 - val_loss: 2.5425\n",
            "Epoch 15/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m911s\u001b[0m 88ms/step - accuracy: 0.9080 - loss: 1.1842 - val_accuracy: 0.7956 - val_loss: 2.4247\n",
            "Epoch 16/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 88ms/step - accuracy: 0.9268 - loss: 1.0546 - val_accuracy: 0.8076 - val_loss: 2.3335\n",
            "Epoch 17/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 88ms/step - accuracy: 0.9404 - loss: 0.9441 - val_accuracy: 0.8171 - val_loss: 2.2542\n",
            "Epoch 18/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 88ms/step - accuracy: 0.9517 - loss: 0.8547 - val_accuracy: 0.8245 - val_loss: 2.1907\n",
            "Epoch 19/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m851s\u001b[0m 83ms/step - accuracy: 0.9605 - loss: 0.7775 - val_accuracy: 0.8323 - val_loss: 2.1290\n",
            "Epoch 20/20\n",
            "\u001b[1m10310/10310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 67ms/step - accuracy: 0.9685 - loss: 0.7094 - val_accuracy: 0.8380 - val_loss: 2.0781\n",
            "GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR): (0.8598903764066744, 0.8598903764066744, 0.8598903764066744, 0.08797806334726514)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 200\n",
        "hidden_units = 400\n",
        "max_sequence_length = 20\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.0007\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build GRU4Rec model\n",
        "def build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "        GRU(hidden_units, return_sequences=False, dropout=0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GRU4Rec model\n",
        "gru4rec_model = build_gru4rec(vocab_size, embedding_dim, hidden_units, max_sequence_length)\n",
        "\n",
        "# Create data generators\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    return dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the model\n",
        "gru4rec_model.fit(train_generator,\n",
        "                  steps_per_epoch=train_steps,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=test_generator,\n",
        "                  validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=10):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate and save results\n",
        "gru4rec_results = evaluate_model_generator(gru4rec_model, test_generator, test_steps)\n",
        "print(\"GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR):\", gru4rec_results)\n",
        "\n",
        "def save_results_to_csv(results, base_path, filename, hyperparameters):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@10', results[0]])\n",
        "        writer.writerow(['Recall@10', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(['Hyperparameters'])\n",
        "        for key, value in hyperparameters.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "# Save the results with hyperparameters\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_units': hidden_units,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'learning_rate': learning_rate\n",
        "}\n",
        "save_results_to_csv(gru4rec_results, base_path, 'gru4rec_results_with_hyperparameters6.csv', hyperparameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoLUqLEefc8X",
        "outputId": "4d1848dd-5573-4ce2-8694-de1aa47f39fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 132ms/step - accuracy: 0.1449 - loss: 7.5790 - val_accuracy: 0.7227 - val_loss: 2.5255\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 122ms/step - accuracy: 0.7770 - loss: 1.8486 - val_accuracy: 0.8639 - val_loss: 1.5499\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 118ms/step - accuracy: 0.9187 - loss: 0.6352 - val_accuracy: 0.8978 - val_loss: 1.2813\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 121ms/step - accuracy: 0.9807 - loss: 0.2051 - val_accuracy: 0.9130 - val_loss: 1.1594\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 131ms/step - accuracy: 0.9999 - loss: 0.0381 - val_accuracy: 0.9264 - val_loss: 1.0740\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9319 - val_loss: 1.0346\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 8.4789e-04 - val_accuracy: 0.9360 - val_loss: 1.0015\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 2.0297e-04 - val_accuracy: 0.9386 - val_loss: 0.9865\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 3.7697e-05 - val_accuracy: 0.9411 - val_loss: 0.9843\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 7.9131e-06 - val_accuracy: 0.9422 - val_loss: 0.9869\n",
            "GRU4Rec Performance (Precision@10, Recall@10, Hit Rate, MRR): (0.9451911878881988, 0.9451911878881988, 0.9451911878881988, 0.09468402623724739)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, MultiHeadAttention, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
        "\n",
        "# Set paths to the CSV files\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path, usecols=['order_id', 'user_id'])\n",
        "order_products_prior = pd.read_csv(order_products_prior_path, usecols=['order_id', 'product_id'])\n",
        "products = pd.read_csv(products_path, usecols=['product_id', 'product_name'])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters to match Script 1\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "max_sequence_length = 100  # Reduced length\n",
        "num_heads = 4\n",
        "ff_dim = 128  # Match hidden units\n",
        "num_layers = 2  # Reduced layers to match Script 1\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Build BERT4Rec model\n",
        "def build_bert4rec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, num_layers, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Stack Transformer encoder layers\n",
        "    x = embeddings\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder(x, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(x[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "bert4rec_model = build_bert4rec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, num_layers, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the BERT4Rec model with early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "bert4rec_model.fit(train_generator,\n",
        "                   steps_per_epoch=train_steps,\n",
        "                   epochs=10,\n",
        "                   validation_data=test_generator,\n",
        "                   validation_steps=test_steps,\n",
        "                   callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    generator_iterator = iter(generator)  # Create an iterator from the dataset\n",
        "    for step in range(steps):\n",
        "        X_batch, y_batch = next(generator_iterator)  # Use the iterator\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate the BERT4Rec model\n",
        "bert4rec_results = evaluate_model_generator(bert4rec_model, test_generator, test_steps)\n",
        "print(\"BERT4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", bert4rec_results)\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(bert4rec_results, base_path, 'bert4rec_results.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmop-q2dNyLJ",
        "outputId": "172e3ca2-b993-475e-bfb9-c780c0c3663a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 153ms/step - accuracy: 0.0086 - loss: 9.2163 - val_accuracy: 0.0123 - val_loss: 8.3871\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 148ms/step - accuracy: 0.0136 - loss: 7.7728 - val_accuracy: 0.0263 - val_loss: 8.2231\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 151ms/step - accuracy: 0.0266 - loss: 7.3841 - val_accuracy: 0.0435 - val_loss: 7.9927\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 167ms/step - accuracy: 0.0464 - loss: 6.9653 - val_accuracy: 0.0817 - val_loss: 7.7525\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m883s\u001b[0m 171ms/step - accuracy: 0.0772 - loss: 6.5650 - val_accuracy: 0.0940 - val_loss: 7.5958\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1039s\u001b[0m 202ms/step - accuracy: 0.1042 - loss: 6.1800 - val_accuracy: 0.1285 - val_loss: 7.3568\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m967s\u001b[0m 188ms/step - accuracy: 0.1352 - loss: 5.7830 - val_accuracy: 0.1813 - val_loss: 7.0109\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1054s\u001b[0m 205ms/step - accuracy: 0.1868 - loss: 5.3299 - val_accuracy: 0.2376 - val_loss: 6.6175\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1057s\u001b[0m 205ms/step - accuracy: 0.2465 - loss: 4.8437 - val_accuracy: 0.2952 - val_loss: 6.2682\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1042s\u001b[0m 202ms/step - accuracy: 0.3087 - loss: 4.3583 - val_accuracy: 0.3567 - val_loss: 5.8322\n",
            "BERT4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR): (0.4637519409937888, 0.4637519409937888, 0.4637519409937888, 0.11492826410452353)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, MultiHeadAttention, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
        "\n",
        "# Set paths to the CSV files\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path, usecols=['order_id', 'user_id'])\n",
        "order_products_prior = pd.read_csv(order_products_prior_path, usecols=['order_id', 'product_id'])\n",
        "products = pd.read_csv(products_path, usecols=['product_id', 'product_name'])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters to match Script 1\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 150\n",
        "max_sequence_length = 100  # Reduced length\n",
        "num_heads = 4\n",
        "ff_dim = 128  # Match hidden units\n",
        "num_layers = 2  # Reduced layers to match Script 1\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Build BERT4Rec model\n",
        "def build_bert4rec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, num_layers, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Stack Transformer encoder layers\n",
        "    x = embeddings\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder(x, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(x[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "bert4rec_model = build_bert4rec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, num_layers, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the BERT4Rec model with early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "bert4rec_model.fit(train_generator,\n",
        "                   steps_per_epoch=train_steps,\n",
        "                   epochs=10,\n",
        "                   validation_data=test_generator,\n",
        "                   validation_steps=test_steps,\n",
        "                   callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    generator_iterator = iter(generator)  # Create an iterator from the dataset\n",
        "    for step in range(steps):\n",
        "        X_batch, y_batch = next(generator_iterator)  # Use the iterator\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate the BERT4Rec model\n",
        "bert4rec_results = evaluate_model_generator(bert4rec_model, test_generator, test_steps)\n",
        "print(\"BERT4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", bert4rec_results)\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(bert4rec_results, base_path, 'bert4rec_results2.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cDPwxOyNlWE",
        "outputId": "7de4ef1c-34cb-48df-f7a9-916cc92b1ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 176ms/step - accuracy: 0.0088 - loss: 9.2702 - val_accuracy: 0.0175 - val_loss: 8.3247\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m919s\u001b[0m 178ms/step - accuracy: 0.0174 - loss: 7.7134 - val_accuracy: 0.0260 - val_loss: 8.0866\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m997s\u001b[0m 193ms/step - accuracy: 0.0271 - loss: 7.3413 - val_accuracy: 0.0400 - val_loss: 8.0138\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1083s\u001b[0m 210ms/step - accuracy: 0.0482 - loss: 7.0065 - val_accuracy: 0.0541 - val_loss: 7.9080\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1080s\u001b[0m 209ms/step - accuracy: 0.0590 - loss: 6.8192 - val_accuracy: 0.0528 - val_loss: 7.9273\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1084s\u001b[0m 210ms/step - accuracy: 0.0666 - loss: 6.6956 - val_accuracy: 0.0533 - val_loss: 7.8810\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1082s\u001b[0m 210ms/step - accuracy: 0.0789 - loss: 6.5704 - val_accuracy: 0.0633 - val_loss: 7.8549\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1085s\u001b[0m 210ms/step - accuracy: 0.0983 - loss: 6.4029 - val_accuracy: 0.0960 - val_loss: 7.6525\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 208ms/step - accuracy: 0.1164 - loss: 6.2684 - val_accuracy: 0.1053 - val_loss: 7.5236\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1086s\u001b[0m 211ms/step - accuracy: 0.1230 - loss: 6.1717 - val_accuracy: 0.0769 - val_loss: 7.6300\n",
            "BERT4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR): (0.19640430900621117, 0.19640430900621117, 0.19640430900621117, 0.059961746247411456)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, MultiHeadAttention, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
        "\n",
        "# Set paths to the CSV files\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path, usecols=['order_id', 'user_id'])\n",
        "order_products_prior = pd.read_csv(order_products_prior_path, usecols=['order_id', 'product_id'])\n",
        "products = pd.read_csv(products_path, usecols=['product_id', 'product_name'])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters to match Script 1\n",
        "vocab_size = products['product_id'].nunique() + 1\n",
        "embedding_dim = 100\n",
        "max_sequence_length = 50\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "num_layers = 2\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Build BERT4Rec model\n",
        "def build_bert4rec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, num_layers, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Stack Transformer encoder layers\n",
        "    x = embeddings\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder(x, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(x[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "bert4rec_model = build_bert4rec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, num_layers, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 64\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Train the BERT4Rec model with early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "bert4rec_model.fit(train_generator,\n",
        "                   steps_per_epoch=train_steps,\n",
        "                   epochs=10,\n",
        "                   validation_data=test_generator,\n",
        "                   validation_steps=test_steps,\n",
        "                   callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    generator_iterator = iter(generator)  # Create an iterator from the dataset\n",
        "    for step in range(steps):\n",
        "        X_batch, y_batch = next(generator_iterator)  # Use the iterator\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate the BERT4Rec model\n",
        "bert4rec_results = evaluate_model_generator(bert4rec_model, test_generator, test_steps)\n",
        "print(\"BERT4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", bert4rec_results)\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(bert4rec_results, base_path, 'bert4rec_results3.csv')\n"
      ],
      "metadata": {
        "id": "bCv7EwRyOeMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad2605a-0823-4b1d-8d36-623b82e7ae82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 151ms/step - accuracy: 0.0198 - loss: 8.7210 - val_accuracy: 0.1260 - val_loss: 5.9951\n",
            "Epoch 2/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 149ms/step - accuracy: 0.1930 - loss: 5.1033 - val_accuracy: 0.3402 - val_loss: 4.4814\n",
            "Epoch 3/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 147ms/step - accuracy: 0.3346 - loss: 3.7417 - val_accuracy: 0.4320 - val_loss: 4.1463\n",
            "Epoch 4/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 147ms/step - accuracy: 0.4305 - loss: 3.0122 - val_accuracy: 0.5086 - val_loss: 3.8335\n",
            "Epoch 5/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 146ms/step - accuracy: 0.5252 - loss: 2.4041 - val_accuracy: 0.5959 - val_loss: 3.4851\n",
            "Epoch 6/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 147ms/step - accuracy: 0.6059 - loss: 1.9214 - val_accuracy: 0.6295 - val_loss: 3.2763\n",
            "Epoch 7/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 147ms/step - accuracy: 0.6528 - loss: 1.6197 - val_accuracy: 0.6541 - val_loss: 3.1422\n",
            "Epoch 8/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 149ms/step - accuracy: 0.6817 - loss: 1.4291 - val_accuracy: 0.6671 - val_loss: 3.0631\n",
            "Epoch 9/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 148ms/step - accuracy: 0.7093 - loss: 1.2697 - val_accuracy: 0.7040 - val_loss: 2.8557\n",
            "Epoch 10/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 150ms/step - accuracy: 0.7459 - loss: 1.0717 - val_accuracy: 0.7144 - val_loss: 2.8061\n",
            "BERT4Rec Performance (Precision@5, Recall@5, Hit Rate, MRR): (0.8027707686335404, 0.8027707686335404, 0.8027707686335404, 0.1773514331003252)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Add\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization based on the i7-13700K capabilities\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function (modify based on dataset specifics)\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    # Merge orders with prior products to get sequences per user\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    # Group by user_id to create sequences of product_ids\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "\n",
        "    # Ensure sequences are not empty, otherwise filter them out\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]  # Take the last item as the label\n",
        "\n",
        "    # Pad sequences to ensure they have the same length\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1  # Example value based on products dataset\n",
        "embedding_dim = 100  # Adjusted for better representation\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 256  # Feed-forward layer dimension\n",
        "dropout_rate = 0.1  # Dropout rate for regularization\n",
        "max_sequence_length = 200  # Example value\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer for SASRec\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    # Multi-head self-attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Function to build SASRec model\n",
        "def build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Add transformer encoder layers (e.g., 2 layers for SASRec)\n",
        "    transformer_output = transformer_encoder(embeddings, num_heads, ff_dim, dropout_rate)\n",
        "    transformer_output = transformer_encoder(transformer_output, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(transformer_output[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the SASRec model\n",
        "sasrec_model = build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32  # Example value\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the SASRec model\n",
        "sasrec_model.fit(train_generator,\n",
        "                 steps_per_epoch=train_steps,\n",
        "                 epochs=10,  # Adjust epochs as needed\n",
        "                 validation_data=test_generator,\n",
        "                 validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function for SASRec model\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate SASRec model\n",
        "sasrec_results = evaluate_model_generator(sasrec_model, test_generator, test_steps)\n",
        "print(\"SASRec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", sasrec_results)\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(sasrec_results, base_path, 'sasrec_resultsO1.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUQLpTnKOP4Z",
        "outputId": "fbe6695f-edd9-4bed-f925-00916fd02ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1514s\u001b[0m 293ms/step - accuracy: 0.0081 - loss: 9.3493 - val_accuracy: 0.0110 - val_loss: 8.6932\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1511s\u001b[0m 293ms/step - accuracy: 0.0120 - loss: 8.1359 - val_accuracy: 0.0117 - val_loss: 8.7496\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1507s\u001b[0m 292ms/step - accuracy: 0.0110 - loss: 7.9894 - val_accuracy: 0.0138 - val_loss: 8.8461\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1504s\u001b[0m 292ms/step - accuracy: 0.0123 - loss: 7.8917 - val_accuracy: 0.0116 - val_loss: 8.9128\n",
            "Epoch 5/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1506s\u001b[0m 292ms/step - accuracy: 0.0124 - loss: 7.8322 - val_accuracy: 0.0145 - val_loss: 8.9884\n",
            "Epoch 6/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1479s\u001b[0m 287ms/step - accuracy: 0.0135 - loss: 7.7571 - val_accuracy: 0.0146 - val_loss: 8.9779\n",
            "Epoch 7/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1497s\u001b[0m 290ms/step - accuracy: 0.0150 - loss: 7.7091 - val_accuracy: 0.0173 - val_loss: 9.0138\n",
            "Epoch 8/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1503s\u001b[0m 292ms/step - accuracy: 0.0154 - loss: 7.6473 - val_accuracy: 0.0159 - val_loss: 9.0240\n",
            "Epoch 9/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1507s\u001b[0m 292ms/step - accuracy: 0.0171 - loss: 7.5828 - val_accuracy: 0.0157 - val_loss: 8.9840\n",
            "Epoch 10/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1508s\u001b[0m 292ms/step - accuracy: 0.0188 - loss: 7.5111 - val_accuracy: 0.0196 - val_loss: 8.9574\n",
            "SASRec Performance (Precision@5, Recall@5, Hit Rate, MRR): (0.05471176242236025, 0.05471176242236025, 0.05471176242236025, 0.020278128234989968)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Add\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization based on the i7-13700K capabilities\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function (modify based on dataset specifics)\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    # Merge orders with prior products to get sequences per user\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    # Group by user_id to create sequences of product_ids\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "\n",
        "    # Ensure sequences are not empty, otherwise filter them out\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]  # Take the last item as the label\n",
        "\n",
        "    # Pad sequences to ensure they have the same length\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1  # Example value based on products dataset\n",
        "embedding_dim = 150  # Adjusted for better representation\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 256  # Feed-forward layer dimension\n",
        "dropout_rate = 0.1  # Dropout rate for regularization\n",
        "max_sequence_length = 100  # Example value\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer for SASRec\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    # Multi-head self-attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Function to build SASRec model\n",
        "def build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Add transformer encoder layers (e.g., 2 layers for SASRec)\n",
        "    transformer_output = transformer_encoder(embeddings, num_heads, ff_dim, dropout_rate)\n",
        "    transformer_output = transformer_encoder(transformer_output, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(transformer_output[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the SASRec model\n",
        "sasrec_model = build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32  # Example value\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the SASRec model\n",
        "sasrec_model.fit(train_generator,\n",
        "                 steps_per_epoch=train_steps,\n",
        "                 epochs=10,  # Adjust epochs as needed\n",
        "                 validation_data=test_generator,\n",
        "                 validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function for SASRec model\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate SASRec model\n",
        "sasrec_results = evaluate_model_generator(sasrec_model, test_generator, test_steps)\n",
        "print(\"SASRec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", sasrec_results)\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(sasrec_results, base_path, 'sasrec_resultsO2.csv')\n"
      ],
      "metadata": {
        "id": "YKlv7YmOO7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c696b0-8e05-4554-8798-810bddf3dd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1379s\u001b[0m 267ms/step - accuracy: 0.0077 - loss: 9.3780 - val_accuracy: 0.0123 - val_loss: 8.9107\n",
            "Epoch 2/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1332s\u001b[0m 258ms/step - accuracy: 0.0131 - loss: 8.2052 - val_accuracy: 0.0141 - val_loss: 8.9171\n",
            "Epoch 3/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1309s\u001b[0m 254ms/step - accuracy: 0.0128 - loss: 7.9949 - val_accuracy: 0.0152 - val_loss: 8.6960\n",
            "Epoch 4/10\n",
            "\u001b[1m5155/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1310s\u001b[0m 254ms/step - accuracy: 0.0150 - loss: 7.7109 - val_accuracy: 0.0176 - val_loss: 8.7005\n",
            "Epoch 5/10\n",
            "\u001b[1m5144/5155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - accuracy: 0.0189 - loss: 7.4412"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Add\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization based on the i7-13700K capabilities\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function (modify based on dataset specifics)\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    # Merge orders with prior products to get sequences per user\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    # Group by user_id to create sequences of product_ids\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "\n",
        "    # Ensure sequences are not empty, otherwise filter them out\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]  # Take the last item as the label\n",
        "\n",
        "    # Pad sequences to ensure they have the same length\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1  # Example value based on products dataset\n",
        "embedding_dim = 100  # Adjusted for better representation\n",
        "num_heads = 2 # Number of attention heads\n",
        "ff_dim = 256  # Feed-forward layer dimension\n",
        "dropout_rate = 0.2  # Dropout rate for regularization\n",
        "max_sequence_length = 50  # Example value\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer for SASRec\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    # Multi-head self-attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Function to build SASRec model\n",
        "def build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Add transformer encoder layers (e.g., 2 layers for SASRec)\n",
        "    transformer_output = transformer_encoder(embeddings, num_heads, ff_dim, dropout_rate)\n",
        "    transformer_output = transformer_encoder(transformer_output, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(transformer_output[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the SASRec model\n",
        "sasrec_model = build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32  # Example value\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the SASRec model\n",
        "sasrec_model.fit(train_generator,\n",
        "                 steps_per_epoch=train_steps,\n",
        "                 epochs=10,  # Adjust epochs as needed\n",
        "                 validation_data=test_generator,\n",
        "                 validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function for SASRec model\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate SASRec model\n",
        "sasrec_results = evaluate_model_generator(sasrec_model, test_generator, test_steps)\n",
        "print(\"SASRec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", sasrec_results)\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(sasrec_results, base_path, 'sasrec_resultsO3.csv')\n"
      ],
      "metadata": {
        "id": "IJ-reJF9MpFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Add\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Adjust TensorFlow settings for CPU optimization based on the i7-13700K capabilities\n",
        "tf.config.threading.set_intra_op_parallelism_threads(16)  # Utilize all available cores\n",
        "tf.config.threading.set_inter_op_parallelism_threads(8)   # Optimize parallel execution between operations\n",
        "\n",
        "# Set paths to the CSV files (update as needed)\n",
        "base_path = r'C:\\Users\\user\\Desktop\\cw\\MBA\\instacart-market-basket-analysis'\n",
        "orders_path = f'{base_path}\\\\orders.csv'\n",
        "order_products_prior_path = f'{base_path}\\\\order_products__prior.csv'\n",
        "products_path = f'{base_path}\\\\products.csv'\n",
        "\n",
        "# Load data from CSV files\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "# Preprocessing function (modify based on dataset specifics)\n",
        "def preprocess_data(orders, order_products, max_sequence_length):\n",
        "    # Merge orders with prior products to get sequences per user\n",
        "    merged = pd.merge(order_products, orders, on='order_id')\n",
        "    # Group by user_id to create sequences of product_ids\n",
        "    sequences = merged.groupby('user_id')['product_id'].apply(list).values\n",
        "\n",
        "    # Ensure sequences are not empty, otherwise filter them out\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) > 1]\n",
        "    labels = [seq[-1] for seq in filtered_sequences]  # Take the last item as the label\n",
        "\n",
        "    # Pad sequences to ensure they have the same length\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequences, np.array(labels)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = products['product_id'].nunique() + 1  # Example value based on products dataset\n",
        "embedding_dim = 100  # Adjusted for better representation\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 256  # Feed-forward layer dimension\n",
        "dropout_rate = 0.2  # Dropout rate for regularization\n",
        "max_sequence_length = 50  # Example value\n",
        "\n",
        "# Preprocess the data\n",
        "X, y = preprocess_data(orders, order_products_prior, max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Positional encoding function\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    pos_encoding = pos * angle_rates\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Transformer Encoder Layer for SASRec\n",
        "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate):\n",
        "    # Multi-head self-attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(ff_output)\n",
        "\n",
        "# Function to build SASRec model\n",
        "def build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate):\n",
        "    inputs = Input(shape=(max_sequence_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = positional_encoding(max_sequence_length, embedding_dim)\n",
        "    embeddings = embedding_layer + pos_encoding\n",
        "\n",
        "    # Add transformer encoder layers (e.g., 2 layers for SASRec)\n",
        "    transformer_output = transformer_encoder(embeddings, num_heads, ff_dim, dropout_rate)\n",
        "    transformer_output = transformer_encoder(transformer_output, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "    # Output layer for prediction\n",
        "    output = Dense(vocab_size, activation='softmax')(transformer_output[:, -1, :])\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the SASRec model\n",
        "sasrec_model = build_sasrec(vocab_size, max_sequence_length, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "# Create data generators for training and testing\n",
        "batch_size = 32  # Example value\n",
        "\n",
        "def create_generator(X, y, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "train_generator = create_generator(X_train, y_train, batch_size)\n",
        "test_generator = create_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Determine steps per epoch\n",
        "train_steps = len(X_train) // batch_size\n",
        "test_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the SASRec model\n",
        "sasrec_model.fit(train_generator,\n",
        "                 steps_per_epoch=train_steps,\n",
        "                 epochs=10,  # Adjust epochs as needed\n",
        "                 validation_data=test_generator,\n",
        "                 validation_steps=test_steps)\n",
        "\n",
        "# Evaluation function for SASRec model\n",
        "def evaluate_model_generator(model, generator, steps, k=5):\n",
        "    precision, recall, hit_rate, mrr, count = 0, 0, 0, 0, 0\n",
        "    for X_batch, y_batch in generator:\n",
        "        predictions = model.predict(X_batch, verbose=0)\n",
        "        top_k_preds = np.argsort(predictions, axis=-1)[:, -k:]\n",
        "        for i in range(len(y_batch)):\n",
        "            true_item = y_batch[i]\n",
        "            rank = np.where(top_k_preds[i] == true_item)[0]\n",
        "            if len(rank) > 0:\n",
        "                rank = rank[0] + 1\n",
        "                precision += 1\n",
        "                recall += 1\n",
        "                hit_rate += 1\n",
        "                mrr += 1 / rank\n",
        "        count += len(y_batch)\n",
        "        if count >= steps * batch_size:\n",
        "            break\n",
        "    precision /= count\n",
        "    recall /= count\n",
        "    hit_rate /= count\n",
        "    mrr /= count\n",
        "    return precision, recall, hit_rate, mrr\n",
        "\n",
        "# Evaluate SASRec model\n",
        "sasrec_results = evaluate_model_generator(sasrec_model, test_generator, test_steps)\n",
        "print(\"SASRec Performance (Precision@5, Recall@5, Hit Rate, MRR):\", sasrec_results)\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save results to a CSV file\n",
        "def save_results_to_csv(results, base_path, filename):\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "        writer.writerow(['Precision@5', results[0]])\n",
        "        writer.writerow(['Recall@5', results[1]])\n",
        "        writer.writerow(['Hit Rate', results[2]])\n",
        "        writer.writerow(['MRR', results[3]])\n",
        "\n",
        "# Example usage\n",
        "save_results_to_csv(sasrec_results, base_path, 'sasrec_resultsO4.csv')\n"
      ],
      "metadata": {
        "id": "GIHNSYsuMyt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFfsm5NdPQv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}