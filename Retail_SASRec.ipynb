{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9kDrmeBShfC",
        "outputId": "093447e2-dcf4-4b2c-8372-c69e3f994e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape: (541909, 8)\n",
            "Cleaned Data Shape: (397924, 8)\n",
            "ItemID range after encoding: 0 to 3876\n",
            "Data Shape after filtering short sequences: (4214, 2)\n",
            "Recalculated Vocabulary Size: 3463\n",
            "X_train shape: (3371, 30), X_test shape: (843, 30)\n",
            "y_train shape: (3371,), y_test shape: (843,)\n",
            "X_train range: 0 to 3462\n",
            "y_train range: 1 to 3460\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing Cell\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "file_path = r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\Online Retail.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name='Online Retail')\n",
        "\n",
        "# Cleaning\n",
        "print(f\"Original Data Shape: {df.shape}\")\n",
        "df_cleaned = df.dropna(subset=['CustomerID', 'Description'])\n",
        "df_cleaned = df_cleaned[df_cleaned['Quantity'] > 0]\n",
        "print(f\"Cleaned Data Shape: {df_cleaned.shape}\")\n",
        "\n",
        "# Encoding\n",
        "df_sorted = df_cleaned.sort_values(by=['CustomerID', 'InvoiceDate'])\n",
        "all_items = df_sorted['Description'].unique()\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(all_items)\n",
        "\n",
        "# Encode ItemID\n",
        "df_sorted['ItemID'] = item_encoder.transform(df_sorted['Description'])\n",
        "print(f\"ItemID range after encoding: {df_sorted['ItemID'].min()} to {df_sorted['ItemID'].max()}\")\n",
        "\n",
        "# Grouping by Customer\n",
        "sequential_data = df_sorted.groupby('CustomerID')['ItemID'].apply(list).reset_index(name='ItemSequence')\n",
        "\n",
        "# Minimum sequence length\n",
        "min_sequence_length = 3\n",
        "sequential_data = sequential_data[sequential_data['ItemSequence'].apply(len) >= min_sequence_length]\n",
        "print(f\"Data Shape after filtering short sequences: {sequential_data.shape}\")\n",
        "\n",
        "# Pad and create sequences\n",
        "item_sequences = sequential_data['ItemSequence'].tolist()\n",
        "sequence_length = 30\n",
        "padded_sequences = pad_sequences(item_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "def create_sequences(sequences, seq_length=30):\n",
        "    X, y = [], []\n",
        "    for seq in sequences:\n",
        "        for i in range(max(1, len(seq) - seq_length + 1)):\n",
        "            X.append(seq[i:i + seq_length])\n",
        "            y.append(seq[min(i + seq_length, len(seq) - 1)])  # Adjust for boundaries\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(padded_sequences, seq_length=sequence_length)\n",
        "\n",
        "# Dataset splitting\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Dynamically calculate the unique item count\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "num_items = len(unique_items)\n",
        "\n",
        "# Fit LabelEncoder on all items in both X and y\n",
        "unique_items = np.unique(np.concatenate([X.flatten(), y]))\n",
        "item_encoder = LabelEncoder()\n",
        "item_encoder.fit(unique_items)\n",
        "\n",
        "# Re-encode X and y to ensure compatibility\n",
        "X_flat = X.flatten()\n",
        "X_encoded = item_encoder.transform(X_flat).reshape(X.shape)\n",
        "y_encoded = item_encoder.transform(y)\n",
        "\n",
        "# Update the dataset\n",
        "X_train, X_test = X_encoded[:split_index], X_encoded[split_index:]\n",
        "y_train, y_test = y_encoded[:split_index], y_encoded[split_index:]\n",
        "\n",
        "# Recalculate vocabulary size\n",
        "num_items = len(item_encoder.classes_)\n",
        "\n",
        "# Debugging information\n",
        "print(f\"Recalculated Vocabulary Size: {num_items}\")\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"X_train range: {X_train.min()} to {X_train.max()}\")\n",
        "print(f\"y_train range: {y_train.min()} to {y_train.max()}\")\n",
        "\n",
        "# Validate data integrity\n",
        "assert X_train.max() < num_items, \"X_train contains indices exceeding vocabulary size!\"\n",
        "assert y_train.max() < num_items, \"y_train contains indices exceeding vocabulary size!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=2,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results.csv\")\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 1\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 2,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 32,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\SASRec_results1.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLTKOY-ISwi5",
        "outputId": "d9d60924-c9ac-463e-9113-a024cc12f86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/106 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   4%|██▏                                                       | 4/106 [00:03<01:01,  1.66it/s, loss=8.144891]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000187B9BE6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   5%|██▋                                                       | 5/106 [00:03<00:53,  1.89it/s, loss=8.142521]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000187B9BE6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|███████████████████████████████████████████████████████| 106/106 [00:44<00:00,  2.36it/s, loss=7.7747364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.9652 - Time: 44.92s\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|███████████████████████████████████████████████████████| 106/106 [00:45<00:00,  2.33it/s, loss=7.2672987]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.3664 - Time: 45.43s\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|███████████████████████████████████████████████████████| 106/106 [00:49<00:00,  2.15it/s, loss=7.0152626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0499 - Time: 49.40s\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|████████████████████████████████████████████████████████| 106/106 [00:50<00:00,  2.08it/s, loss=6.881438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.8929 - Time: 50.95s\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|████████████████████████████████████████████████████████| 106/106 [00:51<00:00,  2.06it/s, loss=6.798756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8009 - Time: 51.53s\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|███████████████████████████████████████████████████████| 106/106 [00:53<00:00,  1.97it/s, loss=6.7384915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7375 - Time: 53.70s\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████| 106/106 [00:55<00:00,  1.91it/s, loss=6.688636]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.6868 - Time: 55.56s\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|███████████████████████████████████████████████████████| 106/106 [00:57<00:00,  1.85it/s, loss=6.6469665]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6444 - Time: 57.26s\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|███████████████████████████████████████████████████████| 106/106 [01:00<00:00,  1.76it/s, loss=6.6106253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.6081 - Time: 60.37s\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████████████████████████████████████████████████| 106/106 [01:01<00:00,  1.71it/s, loss=6.5780797]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.5763 - Time: 61.97s\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6793 - sparse_top_k_categorical_accuracy: 0.0487\n",
            "\n",
            "Test Loss: 8.6643, Test Top-10 Accuracy: 0.0676\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 8.6643\n",
            "Test Top-10 Accuracy: 0.0676\n",
            "Training History:\n",
            "Epoch 1 - Loss: 7.9652 - Time: 44.92s\n",
            "Epoch 2 - Loss: 7.3664 - Time: 45.43s\n",
            "Epoch 3 - Loss: 7.0499 - Time: 49.40s\n",
            "Epoch 4 - Loss: 6.8929 - Time: 50.95s\n",
            "Epoch 5 - Loss: 6.8009 - Time: 51.53s\n",
            "Epoch 6 - Loss: 6.7375 - Time: 53.70s\n",
            "Epoch 7 - Loss: 6.6868 - Time: 55.56s\n",
            "Epoch 8 - Loss: 6.6444 - Time: 57.26s\n",
            "Epoch 9 - Loss: 6.6081 - Time: 60.37s\n",
            "Epoch 10 - Loss: 6.5763 - Time: 61.97s\n",
            "\n",
            "Results saved to sasrec_results.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 1             50          2           2          32   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.1      10    8.66431              0.067616  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Custom metrics: Precision@5, Recall@5, Hit Rate, and MRR\n",
        "def calculate_metrics(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculate Precision@K, Recall@K, Hit Rate, and MRR for a batch.\n",
        "    \"\"\"\n",
        "    precision_at_k, recall_at_k, hit_rate, mrr = 0, 0, 0, 0\n",
        "    batch_size = y_true.shape[0]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        true_label = y_true[i]\n",
        "        top_k_indices = np.argsort(y_pred[i])[-k:][::-1]\n",
        "\n",
        "        if true_label in top_k_indices:\n",
        "            rank = np.where(top_k_indices == true_label)[0][0] + 1\n",
        "            precision_at_k += 1 / k\n",
        "            recall_at_k += 1\n",
        "            hit_rate += 1\n",
        "            mrr += 1 / rank\n",
        "\n",
        "    return (\n",
        "        precision_at_k / batch_size,\n",
        "        recall_at_k / batch_size,\n",
        "        hit_rate / batch_size,\n",
        "        mrr / batch_size,\n",
        "    )\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=32,\n",
        "    num_heads=2,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_top_10_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "y_pred = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "# Calculate additional metrics\n",
        "precision_at_5, recall_at_5, hit_rate, mrr = calculate_metrics(y_test, y_pred, k=5)\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_top_10_accuracy:.4f}\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Recall@5: {recall_at_5:.4f}\")\n",
        "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
        "print(f\"MRR: {mrr:.4f}\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Experiment\": \"Experiment Template\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 4,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 32,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": len(metrics_history),  # Actual number of epochs run\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_top_10_accuracy,\n",
        "    \"Precision@5\": precision_at_5,\n",
        "    \"Recall@5\": recall_at_5,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqH011tVx0k-",
        "outputId": "874f2356-2de6-4c4a-a11f-48a91280eb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/106 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   4%|██▏                                                       | 4/106 [00:01<00:38,  2.63it/s, loss=8.146504]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000022C9A95DFC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   5%|██▋                                                       | 5/106 [00:02<00:33,  3.00it/s, loss=8.145113]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000022C9A95DFC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|█████████████████████████████████████████████████████████| 106/106 [00:27<00:00,  3.90it/s, loss=7.80733]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 8.0078 - Time: 27.15s\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|████████████████████████████████████████████████████████| 106/106 [00:26<00:00,  4.07it/s, loss=7.307621]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.4323 - Time: 26.07s\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|███████████████████████████████████████████████████████| 106/106 [00:27<00:00,  3.87it/s, loss=7.0507174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0960 - Time: 27.43s\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|███████████████████████████████████████████████████████| 106/106 [00:28<00:00,  3.70it/s, loss=6.8992505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.9184 - Time: 28.64s\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|████████████████████████████████████████████████████████| 106/106 [00:29<00:00,  3.53it/s, loss=6.796059]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8047 - Time: 30.00s\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|███████████████████████████████████████████████████████| 106/106 [00:31<00:00,  3.37it/s, loss=6.7204504]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7238 - Time: 31.47s\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████| 106/106 [00:32<00:00,  3.25it/s, loss=6.662267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.6629 - Time: 32.60s\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|████████████████████████████████████████████████████████| 106/106 [00:33<00:00,  3.14it/s, loss=6.615732]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6146 - Time: 33.72s\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|███████████████████████████████████████████████████████| 106/106 [00:35<00:00,  3.01it/s, loss=6.5775185]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.5756 - Time: 35.20s\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████████████████████████████████████████████████| 106/106 [00:36<00:00,  2.89it/s, loss=6.5433745]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.5415 - Time: 36.65s\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████████████████████████████████████████████████| 106/106 [00:38<00:00,  2.78it/s, loss=6.5143127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Loss: 6.5130 - Time: 38.16s\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.68it/s, loss=6.4789643]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Loss: 6.4823 - Time: 39.52s\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████████████████████████████████████████████████| 106/106 [00:40<00:00,  2.59it/s, loss=6.4411106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Loss: 6.4461 - Time: 40.91s\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|███████████████████████████████████████████████████████| 106/106 [00:42<00:00,  2.48it/s, loss=6.399651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Loss: 6.4070 - Time: 42.67s\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████████████████████████████████████████████████| 106/106 [00:43<00:00,  2.42it/s, loss=6.3602166]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Loss: 6.3676 - Time: 43.80s\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████████████████████████████████████████████████| 106/106 [00:45<00:00,  2.34it/s, loss=6.3228197]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Loss: 6.3300 - Time: 45.29s\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████████████████████████████████████████████████| 106/106 [00:46<00:00,  2.29it/s, loss=6.2889934]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Loss: 6.2954 - Time: 46.30s\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████████████████████████████████████████████████| 106/106 [00:48<00:00,  2.19it/s, loss=6.2576337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Loss: 6.2632 - Time: 48.50s\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|███████████████████████████████████████████████████████| 106/106 [00:50<00:00,  2.09it/s, loss=6.230219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 Loss: 6.2343 - Time: 50.76s\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████████████████████████████████████████████████| 106/106 [00:52<00:00,  2.03it/s, loss=6.2074084]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Loss: 6.2096 - Time: 52.12s\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 11.3258\n",
            "Test Top-10 Accuracy: 0.0510\n",
            "Precision@5: 0.0071\n",
            "Recall@5: 0.0356\n",
            "Hit Rate: 0.0356\n",
            "MRR: 0.0283\n",
            "\n",
            "Results saved to sasrec_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Custom metrics: Precision@5, Recall@5, Hit Rate, and MRR\n",
        "def calculate_metrics(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculate Precision@K, Recall@K, Hit Rate, and MRR for a batch.\n",
        "    \"\"\"\n",
        "    precision_at_k, recall_at_k, hit_rate, mrr = 0, 0, 0, 0\n",
        "    batch_size = y_true.shape[0]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        true_label = y_true[i]\n",
        "        top_k_indices = np.argsort(y_pred[i])[-k:][::-1]\n",
        "\n",
        "        if true_label in top_k_indices:\n",
        "            rank = np.where(top_k_indices == true_label)[0][0] + 1\n",
        "            precision_at_k += 1 / k\n",
        "            recall_at_k += 1\n",
        "            hit_rate += 1\n",
        "            mrr += 1 / rank\n",
        "\n",
        "    return (\n",
        "        precision_at_k / batch_size,\n",
        "        recall_at_k / batch_size,\n",
        "        hit_rate / batch_size,\n",
        "        mrr / batch_size,\n",
        "    )\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=3,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=4,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 40\n",
        "batch_size = 32\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_top_10_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "y_pred = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "# Calculate additional metrics\n",
        "precision_at_5, recall_at_5, hit_rate, mrr = calculate_metrics(y_test, y_pred, k=5)\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_top_10_accuracy:.4f}\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Recall@5: {recall_at_5:.4f}\")\n",
        "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
        "print(f\"MRR: {mrr:.4f}\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Experiment\": \"Experiment Template\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 4,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 32,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": len(metrics_history),  # Actual number of epochs run\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_top_10_accuracy,\n",
        "    \"Precision@5\": precision_at_5,\n",
        "    \"Recall@5\": recall_at_5,\n",
        "    \"Hit Rate\": hit_rate,\n",
        "    \"MRR\": mrr\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results13.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results13.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXewm4KtzN87",
        "outputId": "f70d29cb-ad96-4ea7-fc15-24612d13ed93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/106 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   4%|██▎                                                          | 4/106 [00:03<01:01,  1.66it/s, loss=8.145]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000230A7A16200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   5%|██▋                                                       | 5/106 [00:03<00:53,  1.88it/s, loss=8.142586]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000230A7A16200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|███████████████████████████████████████████████████████| 106/106 [00:44<00:00,  2.40it/s, loss=7.7697906]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.9569 - Time: 44.26s\n",
            "Epoch 2/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|████████████████████████████████████████████████████████| 106/106 [00:43<00:00,  2.44it/s, loss=7.260705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.3553 - Time: 43.43s\n",
            "Epoch 3/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|████████████████████████████████████████████████████████| 106/106 [00:46<00:00,  2.27it/s, loss=7.004243]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0382 - Time: 46.66s\n",
            "Epoch 4/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|███████████████████████████████████████████████████████| 106/106 [00:49<00:00,  2.15it/s, loss=6.8801856]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.8902 - Time: 49.41s\n",
            "Epoch 5/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|████████████████████████████████████████████████████████| 106/106 [00:51<00:00,  2.06it/s, loss=6.810267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8099 - Time: 51.39s\n",
            "Epoch 6/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|███████████████████████████████████████████████████████| 106/106 [00:53<00:00,  1.98it/s, loss=6.7585783]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7570 - Time: 53.59s\n",
            "Epoch 7/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|█████████████████████████████████████████████████████████| 106/106 [00:55<00:00,  1.92it/s, loss=6.71315]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.7115 - Time: 55.34s\n",
            "Epoch 8/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|███████████████████████████████████████████████████████| 106/106 [00:57<00:00,  1.83it/s, loss=6.6717916]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6701 - Time: 57.96s\n",
            "Epoch 9/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|████████████████████████████████████████████████████████| 106/106 [00:59<00:00,  1.79it/s, loss=6.635204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.6332 - Time: 59.34s\n",
            "Epoch 10/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████████████████████████████████████████████████| 106/106 [01:01<00:00,  1.71it/s, loss=6.5993505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.6000 - Time: 61.97s\n",
            "Epoch 11/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|███████████████████████████████████████████████████████| 106/106 [01:04<00:00,  1.63it/s, loss=6.559664]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Loss: 6.5640 - Time: 64.97s\n",
            "Epoch 12/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████████████████████████████████████████████████| 106/106 [01:07<00:00,  1.57it/s, loss=6.5195756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Loss: 6.5250 - Time: 67.43s\n",
            "Epoch 13/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████████████████████████████████████████████████| 106/106 [01:09<00:00,  1.52it/s, loss=6.4813776]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Loss: 6.4867 - Time: 69.68s\n",
            "Epoch 14/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|███████████████████████████████████████████████████████| 106/106 [01:12<00:00,  1.47it/s, loss=6.446318]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Loss: 6.4516 - Time: 72.08s\n",
            "Epoch 15/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████████████████████████████████████████████████| 106/106 [01:13<00:00,  1.45it/s, loss=6.4147663]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Loss: 6.4193 - Time: 73.21s\n",
            "Epoch 16/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████████████████████████████████████████████████| 106/106 [01:15<00:00,  1.40it/s, loss=6.3827677]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Loss: 6.3876 - Time: 75.56s\n",
            "Epoch 17/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████████████████████████████████████████████████| 106/106 [01:18<00:00,  1.35it/s, loss=6.3538246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Loss: 6.3580 - Time: 78.42s\n",
            "Epoch 18/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████████████████████████████████████████████████| 106/106 [01:19<00:00,  1.33it/s, loss=6.3251534]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Loss: 6.3291 - Time: 79.57s\n",
            "Epoch 19/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████████████████████████████████████████████████| 106/106 [01:21<00:00,  1.31it/s, loss=6.2980227]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 Loss: 6.3018 - Time: 81.17s\n",
            "Epoch 20/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|████████████████████████████████████████████████████████| 106/106 [01:22<00:00,  1.29it/s, loss=6.27128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Loss: 6.2755 - Time: 82.18s\n",
            "Epoch 21/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|███████████████████████████████████████████████████████| 106/106 [01:25<00:00,  1.24it/s, loss=6.247263]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 Loss: 6.2507 - Time: 85.52s\n",
            "Epoch 22/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|████████████████████████████████████████████████████████| 106/106 [01:27<00:00,  1.21it/s, loss=6.22566]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 Loss: 6.2283 - Time: 87.89s\n",
            "Epoch 23/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████████████████████████████████████████████████| 106/106 [01:30<00:00,  1.18it/s, loss=6.2052603]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 Loss: 6.2077 - Time: 90.03s\n",
            "Epoch 24/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|███████████████████████████████████████████████████████| 106/106 [01:31<00:00,  1.16it/s, loss=6.185765]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 Loss: 6.1882 - Time: 91.57s\n",
            "Epoch 25/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|███████████████████████████████████████████████████████| 106/106 [01:33<00:00,  1.13it/s, loss=6.166077]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 Loss: 6.1688 - Time: 93.98s\n",
            "Epoch 26/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████████████████████████████████████████████████| 106/106 [01:35<00:00,  1.11it/s, loss=6.1451855]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26 Loss: 6.1488 - Time: 95.08s\n",
            "Epoch 27/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|███████████████████████████████████████████████████████| 106/106 [01:38<00:00,  1.07it/s, loss=6.122662]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27 Loss: 6.1272 - Time: 98.69s\n",
            "Epoch 28/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|████████████████████████████████████████████████████████| 106/106 [01:39<00:00,  1.07it/s, loss=6.09729]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28 Loss: 6.1036 - Time: 99.10s\n",
            "Epoch 29/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████████████████████████████████████████████████| 106/106 [01:41<00:00,  1.04it/s, loss=6.0690246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29 Loss: 6.0773 - Time: 101.97s\n",
            "Epoch 30/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████████████████████████████████████████████████| 106/106 [01:42<00:00,  1.03it/s, loss=6.0385866]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30 Loss: 6.0481 - Time: 102.78s\n",
            "Epoch 31/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31: 100%|███████████████████████████████████████████████████████| 106/106 [01:52<00:00,  1.07s/it, loss=6.007878]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31 Loss: 6.0174 - Time: 112.96s\n",
            "Epoch 32/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32: 100%|██████████████████████████████████████████████████████| 106/106 [01:55<00:00,  1.09s/it, loss=5.9768662]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32 Loss: 5.9866 - Time: 115.45s\n",
            "Epoch 33/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33: 100%|███████████████████████████████████████████████████████| 106/106 [01:49<00:00,  1.03s/it, loss=5.946014]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33 Loss: 5.9564 - Time: 109.54s\n",
            "Epoch 34/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34: 100%|██████████████████████████████████████████████████████| 106/106 [01:59<00:00,  1.12s/it, loss=5.9134436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34 Loss: 5.9249 - Time: 119.20s\n",
            "Epoch 35/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35: 100%|██████████████████████████████████████████████████████| 106/106 [02:01<00:00,  1.15s/it, loss=5.8791833]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 35 Loss: 5.8917 - Time: 121.56s\n",
            "Epoch 36/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36: 100%|██████████████████████████████████████████████████████| 106/106 [02:04<00:00,  1.17s/it, loss=5.8460565]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 36 Loss: 5.8583 - Time: 124.26s\n",
            "Epoch 37/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37: 100%|██████████████████████████████████████████████████████| 106/106 [02:06<00:00,  1.19s/it, loss=5.8132205]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 37 Loss: 5.8257 - Time: 126.50s\n",
            "Epoch 38/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38: 100%|██████████████████████████████████████████████████████| 106/106 [02:09<00:00,  1.22s/it, loss=5.7808022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 38 Loss: 5.7935 - Time: 129.06s\n",
            "Epoch 39/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39: 100%|███████████████████████████████████████████████████████| 106/106 [02:12<00:00,  1.25s/it, loss=5.746757]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 39 Loss: 5.7600 - Time: 132.60s\n",
            "Epoch 40/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40: 100%|██████████████████████████████████████████████████████| 106/106 [02:11<00:00,  1.24s/it, loss=5.7103214]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 40 Loss: 5.7249 - Time: 131.96s\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 15.1248\n",
            "Test Top-10 Accuracy: 0.0593\n",
            "Precision@5: 0.0095\n",
            "Recall@5: 0.0474\n",
            "Hit Rate: 0.0474\n",
            "MRR: 0.0351\n",
            "\n",
            "Results saved to sasrec_results13.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=2,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results.csv\")\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 1\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 2,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 32,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\all_experiment_results.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d60924-c9ac-463e-9113-a024cc12f86f",
        "id": "mswxaOOdyFF8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/106 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   4%|██▏                                                       | 4/106 [00:03<01:01,  1.66it/s, loss=8.144891]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000187B9BE6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   5%|██▋                                                       | 5/106 [00:03<00:53,  1.89it/s, loss=8.142521]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000187B9BE6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|███████████████████████████████████████████████████████| 106/106 [00:44<00:00,  2.36it/s, loss=7.7747364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.9652 - Time: 44.92s\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|███████████████████████████████████████████████████████| 106/106 [00:45<00:00,  2.33it/s, loss=7.2672987]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.3664 - Time: 45.43s\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|███████████████████████████████████████████████████████| 106/106 [00:49<00:00,  2.15it/s, loss=7.0152626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0499 - Time: 49.40s\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|████████████████████████████████████████████████████████| 106/106 [00:50<00:00,  2.08it/s, loss=6.881438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.8929 - Time: 50.95s\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|████████████████████████████████████████████████████████| 106/106 [00:51<00:00,  2.06it/s, loss=6.798756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8009 - Time: 51.53s\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|███████████████████████████████████████████████████████| 106/106 [00:53<00:00,  1.97it/s, loss=6.7384915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7375 - Time: 53.70s\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████| 106/106 [00:55<00:00,  1.91it/s, loss=6.688636]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.6868 - Time: 55.56s\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|███████████████████████████████████████████████████████| 106/106 [00:57<00:00,  1.85it/s, loss=6.6469665]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6444 - Time: 57.26s\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|███████████████████████████████████████████████████████| 106/106 [01:00<00:00,  1.76it/s, loss=6.6106253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.6081 - Time: 60.37s\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████████████████████████████████████████████████| 106/106 [01:01<00:00,  1.71it/s, loss=6.5780797]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.5763 - Time: 61.97s\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6793 - sparse_top_k_categorical_accuracy: 0.0487\n",
            "\n",
            "Test Loss: 8.6643, Test Top-10 Accuracy: 0.0676\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 8.6643\n",
            "Test Top-10 Accuracy: 0.0676\n",
            "Training History:\n",
            "Epoch 1 - Loss: 7.9652 - Time: 44.92s\n",
            "Epoch 2 - Loss: 7.3664 - Time: 45.43s\n",
            "Epoch 3 - Loss: 7.0499 - Time: 49.40s\n",
            "Epoch 4 - Loss: 6.8929 - Time: 50.95s\n",
            "Epoch 5 - Loss: 6.8009 - Time: 51.53s\n",
            "Epoch 6 - Loss: 6.7375 - Time: 53.70s\n",
            "Epoch 7 - Loss: 6.6868 - Time: 55.56s\n",
            "Epoch 8 - Loss: 6.6444 - Time: 57.26s\n",
            "Epoch 9 - Loss: 6.6081 - Time: 60.37s\n",
            "Epoch 10 - Loss: 6.5763 - Time: 61.97s\n",
            "\n",
            "Results saved to sasrec_results.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 1             50          2           2          32   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.1      10    8.66431              0.067616  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=2,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results.csv\")\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 1\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 2,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 32,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\all_experiment_results.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d60924-c9ac-463e-9113-a024cc12f86f",
        "id": "tTKpgzhox1wT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/106 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   4%|██▏                                                       | 4/106 [00:03<01:01,  1.66it/s, loss=8.144891]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000187B9BE6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   5%|██▋                                                       | 5/106 [00:03<00:53,  1.89it/s, loss=8.142521]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000187B9BE6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|███████████████████████████████████████████████████████| 106/106 [00:44<00:00,  2.36it/s, loss=7.7747364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.9652 - Time: 44.92s\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|███████████████████████████████████████████████████████| 106/106 [00:45<00:00,  2.33it/s, loss=7.2672987]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.3664 - Time: 45.43s\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|███████████████████████████████████████████████████████| 106/106 [00:49<00:00,  2.15it/s, loss=7.0152626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0499 - Time: 49.40s\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|████████████████████████████████████████████████████████| 106/106 [00:50<00:00,  2.08it/s, loss=6.881438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.8929 - Time: 50.95s\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|████████████████████████████████████████████████████████| 106/106 [00:51<00:00,  2.06it/s, loss=6.798756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8009 - Time: 51.53s\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|███████████████████████████████████████████████████████| 106/106 [00:53<00:00,  1.97it/s, loss=6.7384915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7375 - Time: 53.70s\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████| 106/106 [00:55<00:00,  1.91it/s, loss=6.688636]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.6868 - Time: 55.56s\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|███████████████████████████████████████████████████████| 106/106 [00:57<00:00,  1.85it/s, loss=6.6469665]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6444 - Time: 57.26s\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|███████████████████████████████████████████████████████| 106/106 [01:00<00:00,  1.76it/s, loss=6.6106253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.6081 - Time: 60.37s\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████████████████████████████████████████████████| 106/106 [01:01<00:00,  1.71it/s, loss=6.5780797]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.5763 - Time: 61.97s\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6793 - sparse_top_k_categorical_accuracy: 0.0487\n",
            "\n",
            "Test Loss: 8.6643, Test Top-10 Accuracy: 0.0676\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 8.6643\n",
            "Test Top-10 Accuracy: 0.0676\n",
            "Training History:\n",
            "Epoch 1 - Loss: 7.9652 - Time: 44.92s\n",
            "Epoch 2 - Loss: 7.3664 - Time: 45.43s\n",
            "Epoch 3 - Loss: 7.0499 - Time: 49.40s\n",
            "Epoch 4 - Loss: 6.8929 - Time: 50.95s\n",
            "Epoch 5 - Loss: 6.8009 - Time: 51.53s\n",
            "Epoch 6 - Loss: 6.7375 - Time: 53.70s\n",
            "Epoch 7 - Loss: 6.6868 - Time: 55.56s\n",
            "Epoch 8 - Loss: 6.6444 - Time: 57.26s\n",
            "Epoch 9 - Loss: 6.6081 - Time: 60.37s\n",
            "Epoch 10 - Loss: 6.5763 - Time: 61.97s\n",
            "\n",
            "Results saved to sasrec_results.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 1             50          2           2          32   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.1      10    8.66431              0.067616  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.2\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=4,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results2.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results2.csv\")\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 2\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 4,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 64,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.2,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\all_experiment_results.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwr_YoWBqtXg",
        "outputId": "fff7c96d-ac73-4fd0-9302-61dbee363f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                  | 0/53 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|█████████████████████████████████████████████████████████| 53/53 [00:26<00:00,  1.98it/s, loss=7.8412013]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 8.0357 - Time: 26.82s\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████████████████████████████████████████████████████| 53/53 [00:25<00:00,  2.08it/s, loss=7.349665]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.4980 - Time: 25.50s\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████████████████████████████████████████████████████| 53/53 [00:53<00:00,  1.02s/it, loss=7.100933]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.1512 - Time: 53.86s\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|█████████████████████████████████████████████████████████| 53/53 [00:26<00:00,  2.00it/s, loss=6.9519362]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.9727 - Time: 26.48s\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|█████████████████████████████████████████████████████████| 53/53 [00:29<00:00,  1.82it/s, loss=6.8550973]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8627 - Time: 29.15s\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████████████████████████████████████████████████████| 53/53 [00:30<00:00,  1.74it/s, loss=6.782964]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7848 - Time: 30.45s\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████████████████████████████████████████████████████| 53/53 [00:31<00:00,  1.67it/s, loss=6.724171]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.7244 - Time: 31.72s\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████████████████████████████████████████████████████| 53/53 [00:32<00:00,  1.62it/s, loss=6.673562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6737 - Time: 32.66s\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████████████████████████████████████████████████████| 53/53 [00:34<00:00,  1.53it/s, loss=6.630218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.6296 - Time: 34.74s\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|████████████████████████████████████████████████████████| 53/53 [00:36<00:00,  1.45it/s, loss=6.5934567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.5920 - Time: 36.59s\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|█████████████████████████████████████████████████████████| 53/53 [00:33<00:00,  1.58it/s, loss=6.561796]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Loss: 6.5598 - Time: 33.46s\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|█████████████████████████████████████████████████████████| 53/53 [00:33<00:00,  1.57it/s, loss=6.535671]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Loss: 6.5331 - Time: 33.69s\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|████████████████████████████████████████████████████████| 53/53 [00:34<00:00,  1.52it/s, loss=6.5122705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Loss: 6.5095 - Time: 34.91s\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|█████████████████████████████████████████████████████████| 53/53 [00:35<00:00,  1.49it/s, loss=6.490267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Loss: 6.4882 - Time: 35.46s\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|█████████████████████████████████████████████████████████| 53/53 [00:36<00:00,  1.46it/s, loss=6.467271]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Loss: 6.4675 - Time: 36.41s\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|███████████████████████████████████████████████████████████| 53/53 [00:36<00:00,  1.44it/s, loss=6.4401]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Loss: 6.4430 - Time: 36.90s\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|█████████████████████████████████████████████████████████| 53/53 [00:37<00:00,  1.42it/s, loss=6.415014]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Loss: 6.4171 - Time: 37.33s\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|█████████████████████████████████████████████████████████| 53/53 [00:38<00:00,  1.37it/s, loss=6.385163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Loss: 6.3891 - Time: 38.76s\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|█████████████████████████████████████████████████████████| 53/53 [00:38<00:00,  1.37it/s, loss=6.362783]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 Loss: 6.3641 - Time: 38.57s\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|█████████████████████████████████████████████████████████| 53/53 [00:39<00:00,  1.33it/s, loss=6.337656]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Loss: 6.3407 - Time: 39.80s\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 10.1364 - sparse_top_k_categorical_accuracy: 0.0391\n",
            "\n",
            "Test Loss: 10.2381, Test Top-10 Accuracy: 0.0534\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 10.2381\n",
            "Test Top-10 Accuracy: 0.0534\n",
            "Training History:\n",
            "Epoch 1 - Loss: 8.0357 - Time: 26.82s\n",
            "Epoch 2 - Loss: 7.4980 - Time: 25.50s\n",
            "Epoch 3 - Loss: 7.1512 - Time: 53.86s\n",
            "Epoch 4 - Loss: 6.9727 - Time: 26.48s\n",
            "Epoch 5 - Loss: 6.8627 - Time: 29.15s\n",
            "Epoch 6 - Loss: 6.7848 - Time: 30.45s\n",
            "Epoch 7 - Loss: 6.7244 - Time: 31.72s\n",
            "Epoch 8 - Loss: 6.6737 - Time: 32.66s\n",
            "Epoch 9 - Loss: 6.6296 - Time: 34.74s\n",
            "Epoch 10 - Loss: 6.5920 - Time: 36.59s\n",
            "Epoch 11 - Loss: 6.5598 - Time: 33.46s\n",
            "Epoch 12 - Loss: 6.5331 - Time: 33.69s\n",
            "Epoch 13 - Loss: 6.5095 - Time: 34.91s\n",
            "Epoch 14 - Loss: 6.4882 - Time: 35.46s\n",
            "Epoch 15 - Loss: 6.4675 - Time: 36.41s\n",
            "Epoch 16 - Loss: 6.4430 - Time: 36.90s\n",
            "Epoch 17 - Loss: 6.4171 - Time: 37.33s\n",
            "Epoch 18 - Loss: 6.3891 - Time: 38.76s\n",
            "Epoch 19 - Loss: 6.3641 - Time: 38.57s\n",
            "Epoch 20 - Loss: 6.3407 - Time: 39.80s\n",
            "\n",
            "Results saved to sasrec_results2.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 2             50          4           2          64   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.2      20  10.238118              0.053381  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=4,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 15\n",
        "batch_size = 16\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results5.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results5.csv\")\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 5\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 4,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 16,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\all_experiment_results.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6fc8YI4sFog",
        "outputId": "0d80f3eb-1a3b-4cdb-c2c6-5355e58bdcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/211 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   2%|█                                                         | 4/211 [00:02<01:57,  1.76it/s, loss=8.144838]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001AFCF43A4D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   2%|█▎                                                        | 5/211 [00:03<01:43,  2.00it/s, loss=8.142685]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001AFCF43A4D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|███████████████████████████████████████████████████████| 211/211 [01:27<00:00,  2.41it/s, loss=7.7376595]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.8422 - Time: 87.64s\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|███████████████████████████████████████████████████████| 211/211 [01:33<00:00,  2.26it/s, loss=7.1998444]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.2660 - Time: 93.20s\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|███████████████████████████████████████████████████████| 211/211 [01:49<00:00,  1.93it/s, loss=6.9587073]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 6.9847 - Time: 109.42s\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|████████████████████████████████████████████████████████| 211/211 [01:56<00:00,  1.80it/s, loss=6.816856]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.8326 - Time: 116.94s\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|███████████████████████████████████████████████████████| 211/211 [01:57<00:00,  1.80it/s, loss=6.7197847]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.7378 - Time: 117.24s\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|████████████████████████████████████████████████████████| 211/211 [02:06<00:00,  1.66it/s, loss=6.633724]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.6509 - Time: 126.85s\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████| 211/211 [02:17<00:00,  1.54it/s, loss=6.551238]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.5713 - Time: 137.43s\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|███████████████████████████████████████████████████████| 211/211 [02:24<00:00,  1.46it/s, loss=6.4660606]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.4927 - Time: 144.86s\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|███████████████████████████████████████████████████████| 211/211 [02:34<00:00,  1.36it/s, loss=6.3835144]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.4105 - Time: 154.99s\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|███████████████████████████████████████████████████████| 211/211 [02:42<00:00,  1.30it/s, loss=6.312274]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.3350 - Time: 162.90s\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████████████████████████████████████████████████| 211/211 [02:52<00:00,  1.22it/s, loss=6.2503653]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Loss: 6.2710 - Time: 172.72s\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|███████████████████████████████████████████████████████| 211/211 [03:03<00:00,  1.15it/s, loss=6.190002]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Loss: 6.2115 - Time: 183.13s\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|███████████████████████████████████████████████████████| 211/211 [03:11<00:00,  1.10it/s, loss=6.130194]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Loss: 6.1521 - Time: 191.70s\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|███████████████████████████████████████████████████████| 211/211 [03:21<00:00,  1.05it/s, loss=6.073191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Loss: 6.0940 - Time: 201.51s\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████████████████████████████████████████████████| 211/211 [03:32<00:00,  1.01s/it, loss=6.0235033]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Loss: 6.0415 - Time: 212.41s\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 12.0060 - sparse_top_k_categorical_accuracy: 0.0330\n",
            "\n",
            "Test Loss: 12.4415, Test Top-10 Accuracy: 0.0439\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 12.4415\n",
            "Test Top-10 Accuracy: 0.0439\n",
            "Training History:\n",
            "Epoch 1 - Loss: 7.8422 - Time: 87.64s\n",
            "Epoch 2 - Loss: 7.2660 - Time: 93.20s\n",
            "Epoch 3 - Loss: 6.9847 - Time: 109.42s\n",
            "Epoch 4 - Loss: 6.8326 - Time: 116.94s\n",
            "Epoch 5 - Loss: 6.7378 - Time: 117.24s\n",
            "Epoch 6 - Loss: 6.6509 - Time: 126.85s\n",
            "Epoch 7 - Loss: 6.5713 - Time: 137.43s\n",
            "Epoch 8 - Loss: 6.4927 - Time: 144.86s\n",
            "Epoch 9 - Loss: 6.4105 - Time: 154.99s\n",
            "Epoch 10 - Loss: 6.3350 - Time: 162.90s\n",
            "Epoch 11 - Loss: 6.2710 - Time: 172.72s\n",
            "Epoch 12 - Loss: 6.2115 - Time: 183.13s\n",
            "Epoch 13 - Loss: 6.1521 - Time: 191.70s\n",
            "Epoch 14 - Loss: 6.0940 - Time: 201.51s\n",
            "Epoch 15 - Loss: 6.0415 - Time: 212.41s\n",
            "\n",
            "Results saved to sasrec_results5.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 5             50          4           2          16   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.1      15  12.441521              0.043891  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.1\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=50,\n",
        "    num_heads=4,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001  # Minimum improvement to reset patience\n",
        "best_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "# Training Loop\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "    # Early stopping logic\n",
        "    if epoch_avg_loss < best_loss - min_delta:\n",
        "        best_loss = epoch_avg_loss\n",
        "        early_stop_counter = 0  # Reset patience counter\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "\n",
        "    if early_stop_counter >= patience:\n",
        "        print(f\"\\nEarly stopping triggered. Stopping training at epoch {epoch + 1}.\")\n",
        "        break\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results6.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results6.csv\")\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 6\",\n",
        "    \"Embedding Dim\": 50,\n",
        "    \"Num Heads\": 4,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 32,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.1,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\all_experiment_results.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyFMyz2dsRLf",
        "outputId": "3739e9c0-2fa7-4967-ad75-6790ef79ce0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                 | 0/106 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|███████████████████████████████████████████████████████| 106/106 [00:59<00:00,  1.78it/s, loss=7.7705603]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.9483 - Time: 59.44s\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|█████████████████████████████████████████████████████████| 106/106 [01:00<00:00,  1.76it/s, loss=7.26222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.3535 - Time: 60.15s\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|███████████████████████████████████████████████████████| 106/106 [01:01<00:00,  1.73it/s, loss=7.0050483]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0383 - Time: 61.27s\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|████████████████████████████████████████████████████████| 106/106 [01:03<00:00,  1.66it/s, loss=6.879488]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.8891 - Time: 63.82s\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|███████████████████████████████████████████████████████| 106/106 [01:05<00:00,  1.61it/s, loss=6.8193626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.8177 - Time: 65.67s\n",
            "Epoch 6/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|███████████████████████████████████████████████████████| 106/106 [01:09<00:00,  1.54it/s, loss=6.7739673]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.7713 - Time: 69.04s\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████| 106/106 [01:11<00:00,  1.48it/s, loss=6.732421]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.7297 - Time: 71.47s\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|████████████████████████████████████████████████████████| 106/106 [07:33<00:00,  4.28s/it, loss=6.695413]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.6933 - Time: 453.40s\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|███████████████████████████████████████████████████████| 106/106 [01:10<00:00,  1.51it/s, loss=6.6590004]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.6577 - Time: 70.13s\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████████████████████████████████████████████████| 106/106 [01:10<00:00,  1.51it/s, loss=6.6215925]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.6232 - Time: 70.02s\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|███████████████████████████████████████████████████████| 106/106 [01:19<00:00,  1.33it/s, loss=6.582865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Loss: 6.5872 - Time: 79.47s\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████████████████████████████████████████████████| 106/106 [01:32<00:00,  1.14it/s, loss=6.5428214]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Loss: 6.5488 - Time: 92.80s\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████████████████████████████████████████████████| 106/106 [01:46<00:00,  1.01s/it, loss=6.5044174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Loss: 6.5100 - Time: 106.92s\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████████████████████████████████████████████████| 106/106 [02:00<00:00,  1.13s/it, loss=6.4687023]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Loss: 6.4738 - Time: 120.01s\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████████████████████████████████████████████████| 106/106 [02:14<00:00,  1.26s/it, loss=6.4342747]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Loss: 6.4394 - Time: 134.08s\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|███████████████████████████████████████████████████████| 106/106 [02:27<00:00,  1.39s/it, loss=6.398774]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Loss: 6.4048 - Time: 147.63s\n",
            "Epoch 17/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|███████████████████████████████████████████████████████| 106/106 [02:37<00:00,  1.48s/it, loss=6.366948]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Loss: 6.3728 - Time: 157.32s\n",
            "Epoch 18/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|███████████████████████████████████████████████████████| 106/106 [02:45<00:00,  1.56s/it, loss=6.334467]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Loss: 6.3409 - Time: 165.26s\n",
            "Epoch 19/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|███████████████████████████████████████████████████████| 106/106 [02:49<00:00,  1.60s/it, loss=6.305168]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 Loss: 6.3107 - Time: 169.97s\n",
            "Epoch 20/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████████████████████████████████████████████████| 106/106 [02:51<00:00,  1.62s/it, loss=6.2766414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Loss: 6.2820 - Time: 171.82s\n",
            "Epoch 21/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|███████████████████████████████████████████████████████| 106/106 [02:30<00:00,  1.42s/it, loss=6.251322]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 Loss: 6.2557 - Time: 150.22s\n",
            "Epoch 22/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████████████████████████████████████████████████| 106/106 [02:27<00:00,  1.39s/it, loss=6.2283754]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 Loss: 6.2315 - Time: 147.61s\n",
            "Epoch 23/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████████████████████████████████████████████████| 106/106 [02:31<00:00,  1.43s/it, loss=6.2066107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 Loss: 6.2094 - Time: 151.36s\n",
            "Epoch 24/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|███████████████████████████████████████████████████████| 106/106 [02:35<00:00,  1.46s/it, loss=6.184434]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 Loss: 6.1881 - Time: 155.01s\n",
            "Epoch 25/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████████████████████████████████████████████████| 106/106 [02:40<00:00,  1.52s/it, loss=6.1627135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 Loss: 6.1664 - Time: 160.70s\n",
            "Epoch 26/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████████████████████████████████████████████████| 106/106 [02:43<00:00,  1.54s/it, loss=6.1416774]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26 Loss: 6.1453 - Time: 163.07s\n",
            "Epoch 27/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████████████████████████████████████████████████| 106/106 [02:46<00:00,  1.58s/it, loss=6.1240883]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27 Loss: 6.1271 - Time: 166.98s\n",
            "Epoch 28/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|████████████████████████████████████████████████████████| 106/106 [02:50<00:00,  1.61s/it, loss=6.10462]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28 Loss: 6.1085 - Time: 170.67s\n",
            "Epoch 29/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████████████████████████████████████████████████| 106/106 [02:55<00:00,  1.66s/it, loss=6.0834727]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29 Loss: 6.0885 - Time: 175.66s\n",
            "Epoch 30/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|███████████████████████████████████████████████████████| 106/106 [02:58<00:00,  1.69s/it, loss=6.060505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30 Loss: 6.0669 - Time: 178.66s\n",
            "Epoch 31/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31: 100%|██████████████████████████████████████████████████████| 106/106 [03:01<00:00,  1.71s/it, loss=6.0374956]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31 Loss: 6.0443 - Time: 181.20s\n",
            "Epoch 32/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32: 100%|██████████████████████████████████████████████████████| 106/106 [03:03<00:00,  1.73s/it, loss=6.0136538]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32 Loss: 6.0210 - Time: 183.75s\n",
            "Epoch 33/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33: 100%|███████████████████████████████████████████████████████| 106/106 [03:07<00:00,  1.77s/it, loss=5.987205]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33 Loss: 5.9957 - Time: 187.36s\n",
            "Epoch 34/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34: 100%|██████████████████████████████████████████████████████| 106/106 [03:12<00:00,  1.82s/it, loss=5.9602523]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34 Loss: 5.9691 - Time: 192.59s\n",
            "Epoch 35/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35: 100%|███████████████████████████████████████████████████████| 106/106 [03:14<00:00,  1.84s/it, loss=5.932438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 35 Loss: 5.9418 - Time: 194.88s\n",
            "Epoch 36/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36: 100%|███████████████████████████████████████████████████████| 106/106 [03:16<00:00,  1.86s/it, loss=5.902761]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 36 Loss: 5.9134 - Time: 196.69s\n",
            "Epoch 37/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37: 100%|███████████████████████████████████████████████████████| 106/106 [03:19<00:00,  1.88s/it, loss=5.872174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 37 Loss: 5.8837 - Time: 199.13s\n",
            "Epoch 38/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38: 100%|███████████████████████████████████████████████████████| 106/106 [03:22<00:00,  1.91s/it, loss=5.841152]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 38 Loss: 5.8532 - Time: 202.25s\n",
            "Epoch 39/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39: 100%|██████████████████████████████████████████████████████| 106/106 [03:23<00:00,  1.92s/it, loss=5.8076367]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 39 Loss: 5.8214 - Time: 203.97s\n",
            "Epoch 40/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40: 100%|███████████████████████████████████████████████████████| 106/106 [03:24<00:00,  1.93s/it, loss=5.774022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 40 Loss: 5.7880 - Time: 204.99s\n",
            "Epoch 41/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41: 100%|███████████████████████████████████████████████████████| 106/106 [03:26<00:00,  1.95s/it, loss=5.739516]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 41 Loss: 5.7540 - Time: 206.47s\n",
            "Epoch 42/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42: 100%|██████████████████████████████████████████████████████| 106/106 [03:28<00:00,  1.97s/it, loss=5.7025175]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 42 Loss: 5.7182 - Time: 208.63s\n",
            "Epoch 43/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43: 100%|███████████████████████████████████████████████████████| 106/106 [03:31<00:00,  2.00s/it, loss=5.663516]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 43 Loss: 5.6803 - Time: 211.96s\n",
            "Epoch 44/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44: 100%|███████████████████████████████████████████████████████| 106/106 [03:32<00:00,  2.01s/it, loss=5.622368]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 44 Loss: 5.6403 - Time: 212.71s\n",
            "Epoch 45/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45: 100%|██████████████████████████████████████████████████████| 106/106 [03:36<00:00,  2.04s/it, loss=5.5808506]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 45 Loss: 5.5991 - Time: 216.34s\n",
            "Epoch 46/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46: 100%|██████████████████████████████████████████████████████| 106/106 [03:39<00:00,  2.07s/it, loss=5.5371733]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 46 Loss: 5.5567 - Time: 219.23s\n",
            "Epoch 47/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47: 100%|███████████████████████████████████████████████████████| 106/106 [03:41<00:00,  2.09s/it, loss=5.491518]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 47 Loss: 5.5123 - Time: 221.60s\n",
            "Epoch 48/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48: 100%|███████████████████████████████████████████████████████| 106/106 [03:44<00:00,  2.12s/it, loss=5.445113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 48 Loss: 5.4667 - Time: 224.90s\n",
            "Epoch 49/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49: 100%|██████████████████████████████████████████████████████| 106/106 [03:48<00:00,  2.15s/it, loss=5.3974414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 49 Loss: 5.4199 - Time: 228.18s\n",
            "Epoch 50/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50: 100%|██████████████████████████████████████████████████████| 106/106 [03:51<00:00,  2.18s/it, loss=5.3473253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 50 Loss: 5.3711 - Time: 231.11s\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 238ms/step - loss: 25.7803 - sparse_top_k_categorical_accuracy: 0.0618\n",
            "\n",
            "Test Loss: 26.4220, Test Top-10 Accuracy: 0.0569\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 26.4220\n",
            "Test Top-10 Accuracy: 0.0569\n",
            "Training History:\n",
            "Epoch 1 - Loss: 7.9483 - Time: 59.44s\n",
            "Epoch 2 - Loss: 7.3535 - Time: 60.15s\n",
            "Epoch 3 - Loss: 7.0383 - Time: 61.27s\n",
            "Epoch 4 - Loss: 6.8891 - Time: 63.82s\n",
            "Epoch 5 - Loss: 6.8177 - Time: 65.67s\n",
            "Epoch 6 - Loss: 6.7713 - Time: 69.04s\n",
            "Epoch 7 - Loss: 6.7297 - Time: 71.47s\n",
            "Epoch 8 - Loss: 6.6933 - Time: 453.40s\n",
            "Epoch 9 - Loss: 6.6577 - Time: 70.13s\n",
            "Epoch 10 - Loss: 6.6232 - Time: 70.02s\n",
            "Epoch 11 - Loss: 6.5872 - Time: 79.47s\n",
            "Epoch 12 - Loss: 6.5488 - Time: 92.80s\n",
            "Epoch 13 - Loss: 6.5100 - Time: 106.92s\n",
            "Epoch 14 - Loss: 6.4738 - Time: 120.01s\n",
            "Epoch 15 - Loss: 6.4394 - Time: 134.08s\n",
            "Epoch 16 - Loss: 6.4048 - Time: 147.63s\n",
            "Epoch 17 - Loss: 6.3728 - Time: 157.32s\n",
            "Epoch 18 - Loss: 6.3409 - Time: 165.26s\n",
            "Epoch 19 - Loss: 6.3107 - Time: 169.97s\n",
            "Epoch 20 - Loss: 6.2820 - Time: 171.82s\n",
            "Epoch 21 - Loss: 6.2557 - Time: 150.22s\n",
            "Epoch 22 - Loss: 6.2315 - Time: 147.61s\n",
            "Epoch 23 - Loss: 6.2094 - Time: 151.36s\n",
            "Epoch 24 - Loss: 6.1881 - Time: 155.01s\n",
            "Epoch 25 - Loss: 6.1664 - Time: 160.70s\n",
            "Epoch 26 - Loss: 6.1453 - Time: 163.07s\n",
            "Epoch 27 - Loss: 6.1271 - Time: 166.98s\n",
            "Epoch 28 - Loss: 6.1085 - Time: 170.67s\n",
            "Epoch 29 - Loss: 6.0885 - Time: 175.66s\n",
            "Epoch 30 - Loss: 6.0669 - Time: 178.66s\n",
            "Epoch 31 - Loss: 6.0443 - Time: 181.20s\n",
            "Epoch 32 - Loss: 6.0210 - Time: 183.75s\n",
            "Epoch 33 - Loss: 5.9957 - Time: 187.36s\n",
            "Epoch 34 - Loss: 5.9691 - Time: 192.59s\n",
            "Epoch 35 - Loss: 5.9418 - Time: 194.88s\n",
            "Epoch 36 - Loss: 5.9134 - Time: 196.69s\n",
            "Epoch 37 - Loss: 5.8837 - Time: 199.13s\n",
            "Epoch 38 - Loss: 5.8532 - Time: 202.25s\n",
            "Epoch 39 - Loss: 5.8214 - Time: 203.97s\n",
            "Epoch 40 - Loss: 5.7880 - Time: 204.99s\n",
            "Epoch 41 - Loss: 5.7540 - Time: 206.47s\n",
            "Epoch 42 - Loss: 5.7182 - Time: 208.63s\n",
            "Epoch 43 - Loss: 5.6803 - Time: 211.96s\n",
            "Epoch 44 - Loss: 5.6403 - Time: 212.71s\n",
            "Epoch 45 - Loss: 5.5991 - Time: 216.34s\n",
            "Epoch 46 - Loss: 5.5567 - Time: 219.23s\n",
            "Epoch 47 - Loss: 5.5123 - Time: 221.60s\n",
            "Epoch 48 - Loss: 5.4667 - Time: 224.90s\n",
            "Epoch 49 - Loss: 5.4199 - Time: 228.18s\n",
            "Epoch 50 - Loss: 5.3711 - Time: 231.11s\n",
            "\n",
            "Results saved to sasrec_results6.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 6             50          4           2          32   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.1      50  26.422033               0.05694  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU Optimization Settings\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTRA_OP_PARALLELISM_THREADS'] = '16'\n",
        "os.environ['TF_NUM_INTER_OP_PARALLELISM_THREADS'] = '8'\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SASRec model\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, input_dim, sequence_length, embedding_dim, num_heads, num_blocks):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim, embedding_dim, mask_zero=True)\n",
        "        self.encoder = [\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=num_heads,\n",
        "                key_dim=embedding_dim,\n",
        "                dropout=0.3\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "        self.dense = tf.keras.layers.Dense(input_dim, activation=\"softmax\")\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.encoder:\n",
        "            x = block(x, x)\n",
        "        # Focus on the last timestep for prediction\n",
        "        x = x[:, -1, :]  # Shape becomes (batch_size, embedding_dim)\n",
        "        return self.dense(x)  # Shape becomes (batch_size, input_dim)\n",
        "\n",
        "# Build the SASRec model dynamically\n",
        "model = SASRec(\n",
        "    input_dim=num_items,  # Updated vocabulary size\n",
        "    sequence_length=30,\n",
        "    embedding_dim=200,\n",
        "    num_heads=4,\n",
        "    num_blocks=2\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=[SparseTopKCategoricalAccuracy(k=10)]\n",
        ")\n",
        "\n",
        "# Training Loop\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "metrics_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    steps_done = 0\n",
        "    total_steps = len(X_train) // batch_size + (len(X_train) % batch_size != 0)\n",
        "\n",
        "    with tqdm(total=total_steps, desc=f\"Epoch {epoch + 1}\", dynamic_ncols=True) as pbar:\n",
        "        for step in range(0, len(X_train), batch_size):\n",
        "            batch_x = X_train[step:step + batch_size]\n",
        "            batch_y = y_train[step:step + batch_size]\n",
        "\n",
        "            # Convert to tensors\n",
        "            batch_x = tf.convert_to_tensor(batch_x, dtype=tf.int32)\n",
        "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.int32)\n",
        "\n",
        "            try:\n",
        "                # Train on batch\n",
        "                metrics = model.train_on_batch(batch_x, batch_y)\n",
        "                epoch_loss += metrics[0]\n",
        "\n",
        "                # Update progress dynamically\n",
        "                pbar.set_postfix(loss=metrics[0])\n",
        "                pbar.update(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training at step {steps_done}: {e}\")\n",
        "                break  # Stop training if an error occurs\n",
        "\n",
        "    # End of epoch\n",
        "    elapsed_time = time.time() - start_time\n",
        "    epoch_avg_loss = epoch_loss / total_steps\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {epoch_avg_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "    metrics_history.append({\"Epoch\": epoch + 1, \"Loss\": epoch_avg_loss, \"Time (s)\": elapsed_time})\n",
        "\n",
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print All Metrics\n",
        "print(\"\\n=== Final Metrics ===\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-10 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training History:\")\n",
        "for metric in metrics_history:\n",
        "    print(f\"Epoch {metric['Epoch']} - Loss: {metric['Loss']:.4f} - Time: {metric['Time (s)']:.2f}s\")\n",
        "\n",
        "# Save Results and Model\n",
        "results = {\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\sasrec_results8.csv\", index=False)\n",
        "print(f\"\\nResults saved to sasrec_results8.csv\")\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "all_experiment_results = []\n",
        "\n",
        "# After each experiment, append results\n",
        "experiment_details = {\n",
        "    \"Experiment\": \"Experiment 1\",\n",
        "    \"Embedding Dim\": 200,\n",
        "    \"Num Heads\": 4,\n",
        "    \"Num Blocks\": 2,\n",
        "    \"Batch Size\": 64,\n",
        "    \"Learning Rate\": 0.001,\n",
        "    \"Dropout Rate\": 0.3,\n",
        "    \"Epochs\": epoch + 1,  # Actual number of epochs run (consider early stopping)\n",
        "    \"Test Loss\": test_loss,\n",
        "    \"Test Top-10 Accuracy\": test_accuracy\n",
        "}\n",
        "all_experiment_results.append(experiment_details)\n",
        "\n",
        "# At the end of all experiments\n",
        "results_df = pd.DataFrame(all_experiment_results)\n",
        "results_df.to_csv(r\"C:\\Users\\user\\Desktop\\CW4\\online+retail\\all_experiment_results.csv\", index=False)\n",
        "print(\"\\n=== All Experiment Results ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "3mXUVkQJsyUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356e3486-5e47-4652-f103-104946273f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|                                                                                  | 0/53 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "Epoch 1:   8%|████▍                                                      | 4/53 [00:03<00:33,  1.47it/s, loss=8.124807]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000193F47924D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   9%|█████▌                                                     | 5/53 [00:04<00:28,  1.66it/s, loss=8.102575]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000193F47924D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|█████████████████████████████████████████████████████████| 53/53 [00:27<00:00,  1.91it/s, loss=7.7736883]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Loss: 7.9462 - Time: 27.73s\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████████████████████████████████████████████████████| 53/53 [00:26<00:00,  1.99it/s, loss=7.298389]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Loss: 7.4111 - Time: 26.70s\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████████████████████████████████████████████████████| 53/53 [00:27<00:00,  1.95it/s, loss=7.051483]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Loss: 7.0929 - Time: 27.20s\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|█████████████████████████████████████████████████████████| 53/53 [00:27<00:00,  1.91it/s, loss=6.9736695]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Loss: 6.9824 - Time: 27.71s\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████████████████████████████████████████████████████| 53/53 [00:28<00:00,  1.88it/s, loss=6.919388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Loss: 6.9281 - Time: 28.27s\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|█████████████████████████████████████████████████████████| 53/53 [00:29<00:00,  1.77it/s, loss=6.8633404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Loss: 6.8672 - Time: 29.90s\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|█████████████████████████████████████████████████████████| 53/53 [00:30<00:00,  1.73it/s, loss=6.8204837]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Loss: 6.8212 - Time: 30.62s\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|█████████████████████████████████████████████████████████| 53/53 [00:31<00:00,  1.67it/s, loss=6.7811027]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Loss: 6.7788 - Time: 31.82s\n",
            "Epoch 9/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████████████████████████████████████████████████████| 53/53 [00:32<00:00,  1.65it/s, loss=6.742238]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Loss: 6.7400 - Time: 32.21s\n",
            "Epoch 10/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|████████████████████████████████████████████████████████| 53/53 [00:35<00:00,  1.51it/s, loss=6.6995215]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Loss: 6.7010 - Time: 35.01s\n",
            "Epoch 11/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|████████████████████████████████████████████████████████| 53/53 [00:36<00:00,  1.46it/s, loss=6.6668763]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Loss: 6.6639 - Time: 36.41s\n",
            "Epoch 12/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|████████████████████████████████████████████████████████| 53/53 [00:37<00:00,  1.41it/s, loss=6.6409216]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Loss: 6.6378 - Time: 37.55s\n",
            "Epoch 13/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|█████████████████████████████████████████████████████████| 53/53 [00:39<00:00,  1.35it/s, loss=6.614715]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Loss: 6.6146 - Time: 39.28s\n",
            "Epoch 14/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|████████████████████████████████████████████████████████| 53/53 [00:41<00:00,  1.29it/s, loss=6.5828867]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Loss: 6.5842 - Time: 41.11s\n",
            "Epoch 15/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|█████████████████████████████████████████████████████████| 53/53 [00:42<00:00,  1.25it/s, loss=6.552501]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Loss: 6.5561 - Time: 42.56s\n",
            "Epoch 16/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|█████████████████████████████████████████████████████████| 53/53 [00:45<00:00,  1.18it/s, loss=6.516948]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Loss: 6.5227 - Time: 45.10s\n",
            "Epoch 17/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|█████████████████████████████████████████████████████████| 53/53 [00:46<00:00,  1.13it/s, loss=6.494036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Loss: 6.4972 - Time: 46.96s\n",
            "Epoch 18/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|████████████████████████████████████████████████████████| 53/53 [00:48<00:00,  1.10it/s, loss=6.4745417]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Loss: 6.4747 - Time: 48.10s\n",
            "Epoch 19/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|█████████████████████████████████████████████████████████| 53/53 [00:50<00:00,  1.05it/s, loss=6.452875]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 Loss: 6.4569 - Time: 50.24s\n",
            "Epoch 20/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|█████████████████████████████████████████████████████████| 53/53 [00:51<00:00,  1.04it/s, loss=6.417834]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Loss: 6.4267 - Time: 51.17s\n",
            "Epoch 21/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|████████████████████████████████████████████████████████| 53/53 [00:53<00:00,  1.02s/it, loss=6.3811593]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 Loss: 6.3914 - Time: 53.87s\n",
            "Epoch 22/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|█████████████████████████████████████████████████████████| 53/53 [00:56<00:00,  1.06s/it, loss=6.344971]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 Loss: 6.3559 - Time: 56.37s\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|████████████████████████████████████████████████████████| 53/53 [00:58<00:00,  1.10s/it, loss=6.3013587]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 Loss: 6.3168 - Time: 58.11s\n",
            "Epoch 24/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|████████████████████████████████████████████████████████| 53/53 [00:57<00:00,  1.09s/it, loss=6.2564564]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 Loss: 6.2729 - Time: 57.80s\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|████████████████████████████████████████████████████████| 53/53 [01:02<00:00,  1.18s/it, loss=6.2179494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 Loss: 6.2326 - Time: 62.29s\n",
            "Epoch 26/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|█████████████████████████████████████████████████████████| 53/53 [01:02<00:00,  1.18s/it, loss=6.175307]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26 Loss: 6.1918 - Time: 62.73s\n",
            "Epoch 27/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|█████████████████████████████████████████████████████████| 53/53 [01:05<00:00,  1.24s/it, loss=6.132559]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27 Loss: 6.1485 - Time: 65.74s\n",
            "Epoch 28/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|████████████████████████████████████████████████████████| 53/53 [01:09<00:00,  1.31s/it, loss=6.0896783]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28 Loss: 6.1057 - Time: 69.48s\n",
            "Epoch 29/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|████████████████████████████████████████████████████████| 53/53 [01:11<00:00,  1.36s/it, loss=6.0463448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29 Loss: 6.0621 - Time: 71.94s\n",
            "Epoch 30/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|████████████████████████████████████████████████████████| 53/53 [01:15<00:00,  1.43s/it, loss=6.0030475]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30 Loss: 6.0188 - Time: 75.89s\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 566ms/step - loss: 13.2116 - sparse_top_k_categorical_accuracy: 0.0327\n",
            "\n",
            "Test Loss: 13.3522, Test Top-10 Accuracy: 0.0356\n",
            "\n",
            "=== Final Metrics ===\n",
            "Test Loss: 13.3522\n",
            "Test Top-10 Accuracy: 0.0356\n",
            "Training History:\n",
            "Epoch 1 - Loss: 7.9462 - Time: 27.73s\n",
            "Epoch 2 - Loss: 7.4111 - Time: 26.70s\n",
            "Epoch 3 - Loss: 7.0929 - Time: 27.20s\n",
            "Epoch 4 - Loss: 6.9824 - Time: 27.71s\n",
            "Epoch 5 - Loss: 6.9281 - Time: 28.27s\n",
            "Epoch 6 - Loss: 6.8672 - Time: 29.90s\n",
            "Epoch 7 - Loss: 6.8212 - Time: 30.62s\n",
            "Epoch 8 - Loss: 6.7788 - Time: 31.82s\n",
            "Epoch 9 - Loss: 6.7400 - Time: 32.21s\n",
            "Epoch 10 - Loss: 6.7010 - Time: 35.01s\n",
            "Epoch 11 - Loss: 6.6639 - Time: 36.41s\n",
            "Epoch 12 - Loss: 6.6378 - Time: 37.55s\n",
            "Epoch 13 - Loss: 6.6146 - Time: 39.28s\n",
            "Epoch 14 - Loss: 6.5842 - Time: 41.11s\n",
            "Epoch 15 - Loss: 6.5561 - Time: 42.56s\n",
            "Epoch 16 - Loss: 6.5227 - Time: 45.10s\n",
            "Epoch 17 - Loss: 6.4972 - Time: 46.96s\n",
            "Epoch 18 - Loss: 6.4747 - Time: 48.10s\n",
            "Epoch 19 - Loss: 6.4569 - Time: 50.24s\n",
            "Epoch 20 - Loss: 6.4267 - Time: 51.17s\n",
            "Epoch 21 - Loss: 6.3914 - Time: 53.87s\n",
            "Epoch 22 - Loss: 6.3559 - Time: 56.37s\n",
            "Epoch 23 - Loss: 6.3168 - Time: 58.11s\n",
            "Epoch 24 - Loss: 6.2729 - Time: 57.80s\n",
            "Epoch 25 - Loss: 6.2326 - Time: 62.29s\n",
            "Epoch 26 - Loss: 6.1918 - Time: 62.73s\n",
            "Epoch 27 - Loss: 6.1485 - Time: 65.74s\n",
            "Epoch 28 - Loss: 6.1057 - Time: 69.48s\n",
            "Epoch 29 - Loss: 6.0621 - Time: 71.94s\n",
            "Epoch 30 - Loss: 6.0188 - Time: 75.89s\n",
            "\n",
            "Results saved to sasrec_results8.csv\n",
            "\n",
            "=== All Experiment Results ===\n",
            "     Experiment  Embedding Dim  Num Heads  Num Blocks  Batch Size  \\\n",
            "0  Experiment 1            200          4           2          64   \n",
            "\n",
            "   Learning Rate  Dropout Rate  Epochs  Test Loss  Test Top-10 Accuracy  \n",
            "0          0.001           0.3      30  13.352205              0.035587  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JKbzaNTe-eN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}